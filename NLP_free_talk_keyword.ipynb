{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 閒聊準備工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# import sys\n",
    "# import os\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "\n",
    "class JiebaSegmentor:\n",
    "\n",
    "    def __init__(self, dict_path, userdict=[]):\n",
    "        self.__dict_path = dict_path\n",
    "        self.__userdict = userdict\n",
    "        self.dictionary_init()\n",
    "\n",
    "    def dictionary_init(self):\n",
    "        jieba.set_dictionary(self.__dict_path)\n",
    "        for path in self.__userdict:\n",
    "            print path\n",
    "            jieba.load_userdict(path)\n",
    "\n",
    "    def taiwan_country(self):\n",
    "        return [u'臺北', u'台北', u'基隆', u'臺中', u'台中', u'臺南', u'台南', u'高雄',\n",
    "                u'宜蘭', u'桃園', u'新竹', u'苗栗', u'彰化', u'南投', u'嘉義', u'雲林',\n",
    "                u'屏東', u'臺東', u'台東', u'花蓮', u'澎湖']\n",
    "\n",
    "    def wordToNumber(self, input_text):\n",
    "\n",
    "        target = u''\n",
    "        for s in input_text:\n",
    "\n",
    "            if (s == u'零') or (s == '0'):\n",
    "                to_word = u'0'\n",
    "            elif (s == u'一') or (s == u'壹') or (s == '1'):\n",
    "                to_word = u'1'\n",
    "            elif (s == u'二') or (s == u'兩') or (s == u'貳') or (s == '2'):\n",
    "                to_word = u'2'\n",
    "            elif (s == u'三') or (s == u'參') or (s == '3'):\n",
    "                to_word = u'3'\n",
    "            elif (s == u'四') or (s == u'肆') or (s == '4'):\n",
    "                to_word = u'4'\n",
    "            elif (s == u'五') or (s == u'伍') or (s == '5'):\n",
    "                to_word = u'5'\n",
    "            elif (s == u'六') or (s == u'陸') or (s == '6'):\n",
    "                to_word = u'6'\n",
    "            elif (s == u'七') or (s == u'柒') or (s == '7'):\n",
    "                to_word = u'7'\n",
    "            elif (s == u'八') or (s == u'捌') or (s == '8'):\n",
    "                to_word = u'8'\n",
    "            elif (s == u'九') or (s == u'玖') or (s == '9'):\n",
    "                to_word = u'9'\n",
    "            else:\n",
    "                to_word = s\n",
    "\n",
    "        target = target + to_word\n",
    "        return target\n",
    "\n",
    "    def input_text_preprocessing(self, input_text):\n",
    "\n",
    "        if type(input_text) is not unicode:\n",
    "            input_text = input_text.decode('utf-8')\n",
    "\n",
    "        # input_text = self.wordToNumber(input_text)\n",
    "        return input_text\n",
    "\n",
    "    def get_names(self, input_text):\n",
    "\n",
    "        input_text = self.input_text_preprocessing(input_text)\n",
    "        names = []\n",
    "        words = pseg.cut(input_text)\n",
    "        print words\n",
    "        for w, f in words:\n",
    "            if f.lower() == 'nr':\n",
    "                names.append(w)\n",
    "        for name in names:\n",
    "            print name.encode('utf-8')\n",
    "        return names\n",
    "\n",
    "    def lcut(self, input_text):\n",
    "\n",
    "        input_text = self.input_text_preprocessing(input_text)\n",
    "        cut_raw = jieba.lcut(input_text)\n",
    "        key = []\n",
    "\n",
    "        for k in cut_raw:\n",
    "            key.append(k)\n",
    "        df = pd.DataFrame({\"word\": key})\n",
    "        return df\n",
    "\n",
    "    def pseg_lcut(self, input_text):\n",
    "\n",
    "        input_text = self.input_text_preprocessing(input_text)\n",
    "        cut_raw = pseg.lcut(input_text)\n",
    "        key = []\n",
    "        value = []\n",
    "\n",
    "        for k, v in cut_raw:\n",
    "            tag = v\n",
    "            if k in self.taiwan_country():\n",
    "                tag = u'ns'\n",
    "            if len(k) > 1 and tag == u'x':\n",
    "                tag = u'n'\n",
    "            key.append(k)\n",
    "            value.append(tag)\n",
    "        df = pd.DataFrame({\"word\": key, \"tag\": value})\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/charles/dataset/jieba/dataset_02/dict_taiwan.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /home/charles/dataset/jieba/dataset_02/dict_taiwan.txt ...\n",
      "Loading model from cache /tmp/jieba.uc7e0f43f50d825489eed13ef9e4d17b7.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.uc7e0f43f50d825489eed13ef9e4d17b7.cache\n",
      "Loading model cost 0.487 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.487 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/charles/dataset/jieba/dataset_02/userdict.txt\n",
      "/home/charles/dataset/jieba/dataset_02/dict.txt.big\n",
      "/home/charles/dataset/jieba/dataset_02/dict.txt.small\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v</td>\n",
       "      <td>請問</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t</td>\n",
       "      <td>今天</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v</td>\n",
       "      <td>下雨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>y</td>\n",
       "      <td>嗎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v</td>\n",
       "      <td>請問</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>t</td>\n",
       "      <td>今天</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>v</td>\n",
       "      <td>下雨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>y</td>\n",
       "      <td>嗎</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag word\n",
       "0   v   請問\n",
       "1   t   今天\n",
       "2   v   下雨\n",
       "3   y    嗎\n",
       "4   v   請問\n",
       "5   t   今天\n",
       "6   v   下雨\n",
       "7   y    嗎"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba_dict_path1 = \"/home/charles/dataset/jieba/dataset_02/dict_taiwan.txt\"\n",
    "jieba_dict_path2 = \"/home/charles/dataset/jieba/dataset_02/userdict.txt\"\n",
    "jieba_dict_path3 = \"/home/charles/dataset/jieba/dataset_02/dict.txt.big\"\n",
    "jieba_dict_path4 = \"/home/charles/dataset/jieba/dataset_02/dict.txt.small\"\n",
    "segmentor = JiebaSegmentor(jieba_dict_path1, [jieba_dict_path2,jieba_dict_path3,jieba_dict_path4])\n",
    "df = segmentor.pseg_lcut('請問今天下雨嗎請問今天下雨嗎')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import *\n",
    "from gensim.models import TfidfModel\n",
    "from datetime import datetime\n",
    "from gensim import models\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "class TfidfGensim:\n",
    "\n",
    "    def __init__(self, model_path, dictionary_path, stopword_path=None,\n",
    "                 jieba_dict_path=None, jieba_user_dict_path_list=None, segmentor=None):\n",
    "        self.__model_path = model_path\n",
    "        self.model = None\n",
    "        self.model_init()\n",
    "        self.__stopword_path = stopword_path\n",
    "        self.stopword_set = set()\n",
    "        self.stopword_init()\n",
    "        self.__dictionary_path = dictionary_path\n",
    "        self.dictionary_init()\n",
    "        self.jieba_dict_path = jieba_dict_path\n",
    "        self.jieba_user_dict_path_list = jieba_user_dict_path_list\n",
    "        if segmentor:\n",
    "            self.segmentor = segmentor\n",
    "        else:\n",
    "            self.jieba_init()\n",
    "\n",
    "    def stopword_init(self):\n",
    "        self.stopword_set = set()\n",
    "        if self.__stopword_path:\n",
    "            with open(self.__stopword_path,'r') as stopwords:\n",
    "                for sw in stopwords:\n",
    "                    self.stopword_set.add(sw.strip('\\n').decode('utf-8'))\n",
    "    \n",
    "    def model_init(self):\n",
    "        self.model = TfidfModel.load(self.__model_path)\n",
    "\n",
    "    def dictionary_init(self):\n",
    "        self.dictionary = Dictionary.load(self.__dictionary_path)\n",
    "    \n",
    "    def jieba_init(self):\n",
    "        if not self.jieba_dict_path:\n",
    "            self.segmentor = None\n",
    "            return\n",
    "            \n",
    "        if not self.jieba_user_dict_path_list:\n",
    "            self.jieba_user_dict_path_list = []\n",
    "        \n",
    "        self.segmentor = JiebaSegmentor(self.jieba_dict_path, self.jieba_user_dict_path_list)\n",
    "\n",
    "    def predict(self, input_text, num=10, min_tfidf=0.2):\n",
    "        \n",
    "        if type(input_text) is not unicode:\n",
    "            input_text = input_text.decode('utf-8')\n",
    "        \n",
    "        if not self.segmentor:\n",
    "            return pd.DataFrame({\"word\":[], \"tfidf\":[]})\n",
    "        \n",
    "        words = self.segmentor.pseg_lcut(input_text)\n",
    "        subject_dict={}\n",
    "        for index, row in words.iterrows():\n",
    "#             print row['word'] + ' (' + row['tag'] + ')'\n",
    "            subject_dict.update({row['word']:row['tag']})\n",
    "            \n",
    "        corpus = []\n",
    "        for index, row in words.iterrows():\n",
    "            if row['word'] not in self.stopword_set:\n",
    "                corpus.append(row['word'])\n",
    "            \n",
    "        corpus_key = self.dictionary.doc2bow(corpus)\n",
    "        corpus_tfidf = self.model[corpus_key]\n",
    "        corpus_tfidf = sorted(corpus_tfidf, key=lambda item: item[1], reverse=True)\n",
    "        id2token = dict(zip(self.dictionary.token2id.values(), self.dictionary.token2id.keys()))\n",
    "        key = []\n",
    "        value = []\n",
    "        \n",
    "        corpus_tfidf_len = len(corpus_tfidf)\n",
    "        if corpus_tfidf_len >= num:\n",
    "            corpus_tfidf_len = num\n",
    "        for i in range(corpus_tfidf_len):\n",
    "            \n",
    "            word = id2token[corpus_tfidf[i][0]]\n",
    "            tfidf = corpus_tfidf[i][1]\n",
    "            \n",
    "            if tfidf <= min_tfidf:\n",
    "                break\n",
    "            \n",
    "#             print 'keyword: ' + word + '(' + subject_dict[word] + ') , tfidf: ' + str(tfidf)\n",
    "            key.append(word)\n",
    "            value.append(tfidf)\n",
    "        \n",
    "        df = pd.DataFrame({\"word\":key, \"tfidf\":value})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.694170</td>\n",
       "      <td>請問</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.609174</td>\n",
       "      <td>下雨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.383452</td>\n",
       "      <td>今天</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidf word\n",
       "0  0.694170   請問\n",
       "1  0.609174   下雨\n",
       "2  0.383452   今天"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf_path = \"/home/charles/dataset/tfidf_model/v2\"\n",
    "# tfidf_model_path = tfidf_path+\"/tfidf.model\"\n",
    "# tfidf_dictionary_path = tfidf_path+\"/tfidf_corpus_dict\"\n",
    "# stopwords_path = \"/home/charles/dataset/jieba/stopwords.txt\"\n",
    "\n",
    "# tg = TfidfGensim(model_path=tfidf_model_path, dictionary_path=tfidf_dictionary_path, \n",
    "#                    stopword_path=stopwords_path, segmentor=segmentor)\n",
    "# tg.predict('請問今天下雨嗎')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "class WordToVecGensim:\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.__model_path = model_path\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        self.model = models.Word2Vec.load(self.__model_path)\n",
    "\n",
    "    def predict(self, input_text, num=10, min_confidence=0.2):\n",
    "\n",
    "        if type(input_text) is not unicode:\n",
    "            input_text = input_text.decode('utf-8')\n",
    "\n",
    "#         print 'synonyms : '\n",
    "        key = []\n",
    "        value = []\n",
    "        try:\n",
    "            res = self.model.wv.most_similar(input_text)\n",
    "            for index,item in enumerate(res):\n",
    "                if index == num:\n",
    "                    break\n",
    "                if item[1] <= min_confidence:\n",
    "                    break\n",
    "                \n",
    "                key.append(item[0])\n",
    "                value.append(item[1])\n",
    "#             result.append((word, tfidf))\n",
    "\n",
    "        except KeyError as er:\n",
    "            print 'no synonyms'\n",
    "\n",
    "        df = pd.DataFrame({\"word\":key, \"conf\":value})\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model_path = \"/home/charles/dataset/word2vec_model/v2/word2vec_tw.model\"\n",
    "# # del w2v\n",
    "# w2v = WordToVecGensim(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text: 關雲長\n",
      "synonyms : \n",
      "c: o\n",
      "w: o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "# input_text = '關雲長'\n",
    "# print 'input_text: ' + input_text\n",
    "# res = w2v.predict(input_text)\n",
    "# res = list(res)\n",
    "# for index,item in enumerate(res):\n",
    "#     if index == 5:\n",
    "#         break\n",
    "\n",
    "#     if item[1] < 0.15:\n",
    "#         break\n",
    "#     print item[0] + ': ' + str(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問句分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import regex as re\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class CheckQuestion:\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    class Label(object):\n",
    "        WHY = 'why'\n",
    "        WHEN = 'when'\n",
    "        WHERE = 'where'\n",
    "        WHO = 'who'\n",
    "        WHAT = 'what'\n",
    "        HOW = 'how'\n",
    "        HOW_MANY = 'how_many'\n",
    "        STATUS = 'status'\n",
    "        FEEL = 'feel'\n",
    "        OTHER = 'other'\n",
    "\n",
    "    def get_v_subject(self):\n",
    "        return ['v', 'vd', 'vg', 'vi', 'vn', 'vq', 'vt']\n",
    "\n",
    "    def get_adj_subject(self):\n",
    "        return ['a', 'ad', 'ag', 'an']\n",
    "\n",
    "    def get_people_subject(self):\n",
    "        return ['nr', 'nrfg', 'nrt']\n",
    "\n",
    "    def get_location_subject(self):\n",
    "        return ['ns']\n",
    "\n",
    "    def get_n_subject(self):\n",
    "        return ['n', 'ng', 'nt', 'nz']\n",
    "\n",
    "    def get_eng_subject(self):\n",
    "        return ['eng']\n",
    "\n",
    "    def __get_unit_word(self):\n",
    "        return [u'有沒有', u'什麼', u'哪一個', u'那一個', u'哪個', u'哪些', u'那個', u'那些', u'怎麼', u'哪兒', u'那兒', u'怎', u'哪', u'那']\n",
    "\n",
    "    def get_adj_word_rule(self):\n",
    "        return u'(鬼|怪|神祕|神奇)?(的)?'\n",
    "\n",
    "    def get_unit_word_post_rule(self):\n",
    "\n",
    "        unit_word = self.__get_unit_word()\n",
    "        unit_word_rule = ''\n",
    "\n",
    "        for k, v in enumerate(unit_word):\n",
    "            unit_word_rule = unit_word_rule + v\n",
    "            if k < len(unit_word) - 1:\n",
    "                unit_word_rule = unit_word_rule + '|'\n",
    "\n",
    "        unit_word_rule = '(' + unit_word_rule + ')' + u'(是)?'\n",
    "\n",
    "        return unit_word_rule\n",
    "\n",
    "    def get_unit_word_pre_rule(self):\n",
    "\n",
    "        unit_word = self.__get_unit_word()\n",
    "        unit_word_rule = ''\n",
    "\n",
    "        for k, v in enumerate(unit_word):\n",
    "            unit_word_rule = unit_word_rule + v\n",
    "            if k < len(unit_word) - 1:\n",
    "                unit_word_rule = unit_word_rule + '|'\n",
    "\n",
    "        unit_word_rule = u'(是)?' + '(' + unit_word_rule + ')'\n",
    "\n",
    "        return unit_word_rule\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{class_name}\".format(class_name=self.__class__.__name__)\n",
    "\n",
    "    @abstractmethod\n",
    "    def check(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def label(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def target(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class CheckWhy(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckWhy, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"s\n",
    "        詢問原因:\n",
    "        label: why\n",
    "        句型：\n",
    "        1.(為什麼|爲何|爲啥)\n",
    "        2.通用疑問句+通用形容句+(原因|理由)\n",
    "        3.(原因|理由) + 通用疑問句+通用形容句\n",
    "        \"\"\"\n",
    "\n",
    "        # 1\n",
    "        rule = u'(為什麼|爲何|爲啥)'\n",
    "        pattern = re.compile(rule)\n",
    "        #         print rule\n",
    "        match = pattern.search(input_text)\n",
    "        if match is not None:\n",
    "            self.label = self.Label.WHY\n",
    "            return True\n",
    "\n",
    "        # 2\n",
    "        rule = self.get_unit_word_post_rule() + self.get_adj_word_rule() + u'(原因|理由)'\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        if match is not None:\n",
    "            self.label = self.Label.WHY\n",
    "            return True\n",
    "\n",
    "        # 3\n",
    "        rule = u'(原因|理由)' + self.get_unit_word_pre_rule() + self.get_adj_word_rule()\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        if match is not None:\n",
    "            self.label = self.Label.WHY\n",
    "            return True\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "\n",
    "class CheckWhen(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckWhen, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"\n",
    "        詢問時間:\n",
    "        label: when\n",
    "        \"\"\"\n",
    "\n",
    "        rule = u'(什麼時間|什麼時候|什麼年份|什麼月份|什麼年|什麼月|什麼日|什麼天|' + \\\n",
    "               u'哪時|那時|何時|哪一年|哪一月|哪一天|哪年|哪月|哪日|哪天|那一年|那一月|那一天|那年|那月|那日|那天)'\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        if match is not None:\n",
    "            self.label = self.Label.WHEN\n",
    "            return True\n",
    "        \n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "    \n",
    "class CheckWhere(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckWhere, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"\n",
    "        詢問地點:\n",
    "        label: where\n",
    "        句型：\n",
    "        1.問單一地點\n",
    "        2.(在何處|在何地|何地|何處|在哪裡|哪裡|在哪兒|哪兒|在哪|在那裡|那裡|在那兒|那兒|在那)\n",
    "        3.通用疑問句+通用形容句+(地點|地方|國家|省分|城市|城鎮|{詞性為ns}))\n",
    "        \"\"\"\n",
    "        # 1\n",
    "        cut_df = self.segmentor.pseg_lcut(input_text)\n",
    "        if len(cut_df) == 1 and cut_df['tag'][0] in self.get_location_subject():\n",
    "            self.label = self.Label.WHERE\n",
    "            self.target = cut_df['word'][0]\n",
    "            return True\n",
    "\n",
    "        # 2\n",
    "        rule = u'(在何處|在何地|何地|何處|在哪裡|哪裡|在哪兒|哪兒|在哪|在那裡|那裡|在那兒|那兒|在那)'\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        if match is not None:\n",
    "            for index, row in cut_df.iterrows():\n",
    "                w = row['word']\n",
    "                n = row['tag']\n",
    "                if n.lower() in self.get_location_subject():\n",
    "                    self.label = self.Label.WHERE\n",
    "                    self.target = w\n",
    "                    return True\n",
    "\n",
    "            self.label = self.Label.WHERE\n",
    "            return True\n",
    "\n",
    "        # 3\n",
    "        rule = self.get_unit_word_post_rule() + \\\n",
    "               self.get_adj_word_rule() + \\\n",
    "               u'(地點|地方|國家|省分|城市|城鎮)'\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        # print match\n",
    "        if match is not None:\n",
    "            self.label = self.Label.WHERE\n",
    "            return True\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "\n",
    "class CheckWho(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckWho, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"\n",
    "        詢問人:\n",
    "        label: who\n",
    "        句型：\n",
    "        1.問單一人名\n",
    "        2.(誰|哪位|那位)\n",
    "        3.通用疑問句+通用形容句+(人)\n",
    "        4.通用疑問句+通用形容句+{詞性為nr}\n",
    "        5.{詞性為nr}+通用疑問句+通用形容句\n",
    "        \"\"\"\n",
    "        # 1\n",
    "        cut_df = self.segmentor.pseg_lcut(input_text)\n",
    "        if len(cut_df) == 1 and cut_df['tag'][0] in self.get_people_subject():\n",
    "            self.label = self.Label.WHO\n",
    "            self.target = cut_df['word'][0]\n",
    "            return True\n",
    "\n",
    "        # 2\n",
    "        rule = u'(誰|哪位|那位)'\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        # print match\n",
    "        if match is not None:\n",
    "            for index, row in cut_df.iterrows():\n",
    "                w = row['word']\n",
    "                n = row['tag']\n",
    "                #                 print w\n",
    "                #                 print n\n",
    "                if n.lower() in self.get_people_subject():\n",
    "                    self.label = self.Label.WHO\n",
    "                    self.target = w\n",
    "                    return True\n",
    "\n",
    "            self.label = self.Label.WHO\n",
    "            return True\n",
    "\n",
    "        # 3\n",
    "        rule = self.get_unit_word_post_rule() + \\\n",
    "               self.get_adj_word_rule() + \\\n",
    "               u'(人)'\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        # print match\n",
    "        if match is not None:\n",
    "            self.label = self.Label.WHO\n",
    "            return True\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "\n",
    "class CheckWhat(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckWhat, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"\n",
    "        詢問一般事務:\n",
    "        label: what\n",
    "        句型：\n",
    "        1.單一個字{詞性為名詞}\n",
    "        2.(什麼是|哪個是|那個是)+{詞性為名詞}\n",
    "        3.{詞性為名詞}+(是什麼|是哪個|是那個)\n",
    "\n",
    "        \"\"\"\n",
    "        # 1\n",
    "        cut_df = self.segmentor.pseg_lcut(input_text)\n",
    "        if len(cut_df) == 1 and cut_df['tag'][0] in self.get_n_subject():\n",
    "            self.label = self.Label.WHAT\n",
    "            self.target = cut_df['word'][0]\n",
    "            return True\n",
    "\n",
    "        # 2\n",
    "        rule = u'(什麼是|哪個是|那個是)(.*)'\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "\n",
    "        if match is not None:\n",
    "            #             print match.group(1)\n",
    "            #             print match.group(2)\n",
    "\n",
    "            content = None\n",
    "            if match.group(1):\n",
    "                content = match.group(1)\n",
    "\n",
    "            if match.group(2):\n",
    "                content = match.group(2)\n",
    "\n",
    "            if content is not None:\n",
    "                cut_df = self.segmentor.pseg_lcut(content)\n",
    "                #                 print 'cut_df'\n",
    "                #                 print cut_df\n",
    "                for index, row in cut_df.iterrows():\n",
    "                    w = row['word']\n",
    "                    n = row['tag']\n",
    "                    #                     if n.lower() not in self.get_people_subject() and \\\n",
    "                    #                             n.lower() not in self.get_location_subject():\n",
    "                    if n.lower() in self.get_n_subject():\n",
    "                        self.label = self.Label.WHAT\n",
    "                        self.target = w\n",
    "                        return True\n",
    "\n",
    "        # 3\n",
    "        rule = u'(.*)(是什麼|是哪個|是那個)'\n",
    "        #         print rule\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "\n",
    "        if match is not None:\n",
    "            content = None\n",
    "            if match.group(1):\n",
    "                content = match.group(1)\n",
    "\n",
    "            if content is not None:\n",
    "                cut_df = self.segmentor.pseg_lcut(content)\n",
    "                for index, row in cut_df.iterrows():\n",
    "                    w = row['word']\n",
    "                    n = row['tag']\n",
    "                    #                     print w\n",
    "                    #                     print n\n",
    "                    #                     if n.lower() not in self.get_people_subject() and \\\n",
    "                    #                             n.lower() not in self.get_location_subject():\n",
    "                    if n.lower() in self.get_n_subject():\n",
    "                        self.label = self.Label.WHAT\n",
    "                        self.target = w\n",
    "                        return True\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "\n",
    "class CheckHow(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckHow, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"\n",
    "        詢問動作:\n",
    "        label: how\n",
    "        句型：\n",
    "        1.(怎麼|怎樣|如何|教我|教導我|請問)+動詞\n",
    "        \"\"\"\n",
    "\n",
    "        # 1\n",
    "        rule = u'(怎麼|怎樣|如何|教我|教導我|學|學會)(.*)'\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "\n",
    "        if match is not None:\n",
    "            #             print match.group(1)\n",
    "            #             print match.group(2)\n",
    "\n",
    "            content = match.group(2)\n",
    "            if content is not None:\n",
    "                cut_df = self.segmentor.pseg_lcut(content)\n",
    "                for index, row in cut_df.iterrows():\n",
    "                    w = row['word']\n",
    "                    n = row['tag']\n",
    "                    if n.lower() in self.get_v_subject():\n",
    "                        self.label = self.Label.HOW\n",
    "                        self.target = input_text\n",
    "                        return True\n",
    "                    break\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "    \n",
    "class CheckHowMany(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckHowMany, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"\n",
    "        詢問數字:\n",
    "        label: howmany\n",
    "        句型：\n",
    "        1.(多少|幾)\n",
    "        \"\"\"\n",
    "\n",
    "        # 1\n",
    "        rule = u'(多少|幾)'\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "        #         print rule\n",
    "        if match is not None:\n",
    "            self.label = self.Label.HOW_MANY\n",
    "            return True\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "class CheckStatus(CheckQuestion):\n",
    "\n",
    "    def __init__(self, segmentor):\n",
    "        super(CheckStatus, self).__init__()\n",
    "        self.label = ''\n",
    "        self.target = ''\n",
    "        self.segmentor = segmentor\n",
    "\n",
    "    def check(self, input_text):\n",
    "        \"\"\"\n",
    "        詢問狀態:\n",
    "        label: what status\n",
    "        句型：\n",
    "        1.多 + 形容詞\n",
    "        \"\"\"\n",
    "\n",
    "        # 1\n",
    "        rule = u'(多)(.*)'\n",
    "        pattern = re.compile(rule)\n",
    "        match = pattern.search(input_text)\n",
    "\n",
    "        if match is not None:\n",
    "            #             print match.group(1)\n",
    "            #             print match.group(2)\n",
    "            content = match.group(2)\n",
    "            if content is not None:\n",
    "                cut_df = self.segmentor.pseg_lcut(content)\n",
    "                for index, row in cut_df.iterrows():\n",
    "                    w = row['word']\n",
    "                    n = row['tag']\n",
    "                    if n.lower() in self.get_adj_subject():\n",
    "                        self.label = self.Label.STATUS\n",
    "                        return True\n",
    "                    break\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "class CheckOther(CheckQuestion):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CheckOther, self).__init__()\n",
    "        self.label = 'other'\n",
    "        self.target = ''\n",
    "\n",
    "    def check(self, input_text):\n",
    "        return True\n",
    "\n",
    "    def label(self):\n",
    "        return self.label\n",
    "\n",
    "    def target(self):\n",
    "        return self.target\n",
    "\n",
    "\n",
    "class QuestionTypeCheck:\n",
    "\n",
    "    def __init__(self, segmentor, source='user'):\n",
    "        self.segmentor = segmentor\n",
    "        self.source = source\n",
    "        self.check_pipeline = [CheckWhy(self.segmentor),\n",
    "                               CheckWhen(self.segmentor),\n",
    "                               CheckWhere(self.segmentor),\n",
    "                               CheckWho(self.segmentor),\n",
    "                               CheckHow(self.segmentor),\n",
    "                               CheckHowMany(self.segmentor),\n",
    "                               CheckStatus(self.segmentor),\n",
    "                               CheckWhat(self.segmentor)]\n",
    "        #                       CheckFeel(self.segmentor)]\n",
    "\n",
    "    QuestionTypeTuple = namedtuple('QuestionTypeTuple', {\n",
    "        'label',\n",
    "        'target',\n",
    "    })\n",
    "\n",
    "    @property\n",
    "    def source(self):\n",
    "        return self.source\n",
    "\n",
    "    @source.setter\n",
    "    def source(self, source):\n",
    "        self.source = source\n",
    "\n",
    "    def check_question_type(self, input_text):\n",
    "        \"\"\"\n",
    "        [ Command ] design pattern\n",
    "        \"\"\"\n",
    "        if type(input_text) is not unicode:\n",
    "            input_text = input_text.decode('utf-8')\n",
    "\n",
    "        if self.source == 'ptt':\n",
    "            # ptt text clean\n",
    "            input_text = input_text.replace(u'Re: ', u'')\n",
    "            input_text = re.sub(u\"\\\\[.*?]\", u\"\", input_text)\n",
    "\n",
    "        label = ''\n",
    "        target = ''\n",
    "\n",
    "        # 有被標注到就不往下做\n",
    "        for cmd in self.check_pipeline:\n",
    "            if cmd.check(input_text):\n",
    "                label = '#' + cmd.label\n",
    "                target = cmd.target\n",
    "                break\n",
    "\n",
    "        if label == '':\n",
    "            cmd = CheckOther()\n",
    "            cmd.check(input_text)\n",
    "            label = '#' + cmd.label\n",
    "\n",
    "        qt = self.QuestionTypeTuple(label=label,\n",
    "                                    target=target)\n",
    "        print 'label = ' + qt.label\n",
    "        print 'target = ' + qt.target\n",
    "        return qt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPEN API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import functools\n",
    "from functools import wraps\n",
    "import regex as re\n",
    "\n",
    "def cache(func):\n",
    "    memo = {}\n",
    "\n",
    "    @wraps(func)\n",
    "    def _wrapper(*args):\n",
    "        res = memo.get(args, None)\n",
    "        if res is not None:\n",
    "            print 'exist'\n",
    "            return res\n",
    "        else:\n",
    "            res = func(*args)\n",
    "            memo[args] = res\n",
    "        return res\n",
    "    return _wrapper\n",
    "\n",
    "class WikiPedia:\n",
    "    \n",
    "    def __init__(self, lang='zh'):\n",
    "        self.__lang = lang\n",
    "   \n",
    "    def set_lang(self, lang):\n",
    "        self.__lang = lang\n",
    "\n",
    "    def wiki_query_list(self, query, lang):\n",
    "        \"\"\"\n",
    "        查詢wiki單字解釋與相關詞API\n",
    "        \"\"\"\n",
    "\n",
    "        url = (\"https://\"+lang+\".wikipedia.org/w/api.php?action=opensearch&search=\"+query+\"&utf8\")    \n",
    "        response = requests.post(url)\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            result = response.json()\n",
    "        #     print result[0]\n",
    "            if len(result[2]) == 0:\n",
    "                return None\n",
    "\n",
    "            content = result[2][0]\n",
    "            if content == u\"\" or u\"重定向\" in content:  \n",
    "                print 'redirect'\n",
    "                return None\n",
    "\n",
    "            return content\n",
    "        \n",
    "    def wiki_query_snippet(self, query, lang):\n",
    "    \n",
    "        def query_api(query_str, lang):\n",
    "            \n",
    "            search_list = []\n",
    "            url = (\"https://\"+lang+\".wikipedia.org/w/api.php?action=query&list=search&srsearch=\"+\n",
    "                    query_str+\"&format=json&formatversion=2\")    \n",
    "            response = requests.post(url)\n",
    "            if response.status_code == requests.codes.ok:\n",
    "\n",
    "                result = response.json()\n",
    "                if \"query\" in result:\n",
    "                    if \"search\" in result[\"query\"]:\n",
    "                        search_list = result[\"query\"][\"search\"]\n",
    "\n",
    "            return search_list\n",
    "\n",
    "        search_list = query_api(query, lang)\n",
    "        if search_list:\n",
    "\n",
    "            if search_list[0].has_key(\"title\"):\n",
    "                title = search_list[0][\"title\"]\n",
    "                search_list = query_api(title, lang)  \n",
    "                if search_list:\n",
    "                    if search_list[0].has_key(\"snippet\") and\\\n",
    "                        search_list[0][\"snippet\"]:\n",
    "\n",
    "                        snippet = search_list[0][\"snippet\"]\n",
    "                        snippet = snippet.split(u'。')[0]\n",
    "                        snippet = re.sub(r'</?\\w+[^>]*>','',snippet)+u'。'\n",
    "                        return snippet\n",
    "        return None\n",
    "    \n",
    "    @cache\n",
    "    def summery(self, query):\n",
    "        \"\"\"\n",
    "        查詢wiki API\n",
    "        1.查詢wiki單字解釋與相關詞API\n",
    "        2.查詢wiki單字解釋片段API\n",
    "        都以中文wiki百科查詢, 當出現重定向(縮寫或只有簡體文章時發生), 改由查詢wiki單字解釋片段API\n",
    "        wiki單字解釋片段API的答案不完整, 所以不考慮一開始就呼叫\n",
    "        爲了提高英文單字準確性, 會使用wiki單字解釋片段API查出的title再當單字查詢一次\n",
    "        \"\"\"\n",
    "\n",
    "        def check_contain_chinese(query):\n",
    "\n",
    "            for ch in query:\n",
    "                if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        if type(query) is not unicode:\n",
    "            query = query.decode('utf-8')\n",
    "\n",
    "    #     if check_contain_chinese(query):\n",
    "    #         lang = 'zh'\n",
    "    #     else:\n",
    "    #         lang = 'en'\n",
    "\n",
    "        lang = 'zh'\n",
    "\n",
    "        print 'wiki query : ' + query\n",
    "        for result in [self.wiki_query_list(query, lang), \n",
    "                       self.wiki_query_snippet(query, lang)]:\n",
    "            if result:\n",
    "                return result\n",
    "        return None\n",
    "\n",
    "class GoogleSearch:\n",
    "    \n",
    "    def __init__(self, lang='lang_zh-TW'):\n",
    "        self.lang = lang\n",
    "        self.url = 'https://www.google.com.tw/search'\n",
    "    \n",
    "    def url(self):\n",
    "        return self.url\n",
    "    \n",
    "    def lang(self):\n",
    "        return self.lang\n",
    "\n",
    "    def search(self, query):\n",
    "        if type(query) is not unicode:\n",
    "            query = query.decode('utf-8')\n",
    "\n",
    "        # 查詢參數\n",
    "        my_params = {'q': query, 'lr':self.lang}\n",
    "\n",
    "        # 下載 Google 搜尋結果\n",
    "        r = requests.get(self.url, params = my_params)\n",
    "\n",
    "        # 確認是否下載成功\n",
    "        if r.status_code == requests.codes.ok:\n",
    "          # 以 BeautifulSoup 解析 HTML 原始碼\n",
    "          soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "          # 觀察 HTML 原始碼\n",
    "          # print(soup.prettify())\n",
    "\n",
    "          # 以 CSS 的選擇器來抓取 Google 的搜尋結果\n",
    "          items = soup.select('div.g > h3.r > a[href^=\"/url\"]')\n",
    "          for i in items[:5]:\n",
    "            # 標題\n",
    "            print(u\"標題：\" + i.text)\n",
    "            # 網址\n",
    "            print(u\"網址：\" + i.get('href').lstrip('/url?q='))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 閒聊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/charles/dataset/jieba/dict_taiwan.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /home/charles/dataset/jieba/dict_taiwan.txt ...\n",
      "Loading model from cache /tmp/jieba.u48306fa201322dcccc3d0c62898fbadc.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.u48306fa201322dcccc3d0c62898fbadc.cache\n",
      "Loading model cost 0.833 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.833 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/charles/dataset/jieba/userdict.txt\n",
      "/home/charles/dataset/jieba/dict.txt.big\n",
      "/home/charles/dataset/jieba/dict.txt.small\n"
     ]
    }
   ],
   "source": [
    "jieba_dict_path1 = \"/home/charles/dataset/jieba/dict_taiwan.txt\"\n",
    "jieba_dict_path2 = \"/home/charles/dataset/jieba/userdict.txt\"\n",
    "jieba_dict_path3 = \"/home/charles/dataset/jieba/dict.txt.big\"\n",
    "jieba_dict_path4 = \"/home/charles/dataset/jieba/dict.txt.small\"\n",
    "del segmentor\n",
    "segmentor = JiebaSegmentor(jieba_dict_path1, [jieba_dict_path2,jieba_dict_path3,jieba_dict_path4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_path = \"/home/charles/dataset/tfidf_model/v2\"\n",
    "tfidf_model_path = tfidf_path+\"/tfidf.model\"\n",
    "tfidf_dictionary_path = tfidf_path+\"/tfidf_corpus_dict\"\n",
    "stopwords_path = \"/home/charles/dataset/jieba/stopwords.txt\"\n",
    "\n",
    "del tg\n",
    "tg = TfidfGensim(model_path=tfidf_model_path, dictionary_path=tfidf_dictionary_path, \n",
    "                   stopword_path=stopwords_path, segmentor=segmentor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_path = \"/home/charles/dataset/word2vec_model/v2/word2vec_tw.model\"\n",
    "del w2v\n",
    "w2v = WordToVecGensim(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuestionTypeCheck(segmentor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = WikiPedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ns</td>\n",
       "      <td>臺中</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>今天天氣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>晴朗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s</td>\n",
       "      <td>路邊</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n</td>\n",
       "      <td>野花</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ns</td>\n",
       "      <td>香</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tag  word\n",
       "0  ns    臺中\n",
       "1   i  今天天氣\n",
       "2   a     好\n",
       "3   a    晴朗\n",
       "4   x     ,\n",
       "5   s    路邊\n",
       "6   n    野花\n",
       "7  ns     香"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = '臺中今天天氣好晴朗,路邊野花香'\n",
    "df = segmentor.pseg_lcut(ww)\n",
    "df\n",
    "# for index, row in df.iterrows():\n",
    "#     print row['word']+ '(' + row['tag']+ ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.817155</td>\n",
       "      <td>晴朗</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.506558</td>\n",
       "      <td>今天天氣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.169780</td>\n",
       "      <td>野花</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidf  word\n",
       "0  0.817155    晴朗\n",
       "1  0.506558  今天天氣\n",
       "2  0.169780    野花"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = u'今天天氣好晴朗,今天天氣好晴朗,路邊野花香,晴朗,晴朗,晴朗'\n",
    "df = tg.predict(ww, num=8, min_tfidf=0.15)\n",
    "df\n",
    "# for item in res:\n",
    "#     print item[0] + '(' + str(item[1]) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conf</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.627519</td>\n",
       "      <td>女明星</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.615591</td>\n",
       "      <td>帥哥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.594882</td>\n",
       "      <td>美腿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.592432</td>\n",
       "      <td>童顏</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.587876</td>\n",
       "      <td>美胸</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       conf word\n",
       "0  0.627519  女明星\n",
       "1  0.615591   帥哥\n",
       "2  0.594882   美腿\n",
       "3  0.592432   童顏\n",
       "4  0.587876   美胸"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = w2v.predict(u'美女', num=5, min_confidence=0.5)\n",
    "df\n",
    "# for item in res:\n",
    "#     print item[0] + '(' + str(item[1]) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(input_text):\n",
    "    \n",
    "    # 問句分類\n",
    "    question_type = qt.check_question_type(input_text)\n",
    "    \n",
    "    if question_type.label in ['what', 'where', 'who']:\n",
    "        wp = WikiPedia()\n",
    "        print wp.summery(qt.target)\n",
    "        return\n",
    "    elif question_type.label in ['how']:\n",
    "        # google整句丟入\n",
    "        google_search(qt.target)\n",
    "        return\n",
    "        \n",
    "    # 斷詞\n",
    "    cut_df = segmentor.pseg_lcut(input_text)\n",
    "    tag_dict = {}\n",
    "    for index, row in cut_df.iterrows():\n",
    "        tag_dict.update({row['word']:row['tag']})\n",
    "\n",
    "    # 找關鍵字\n",
    "    keyword_df = tg.predict(input_text, min_tfidf=0.15)\n",
    "    \n",
    "    # TFIDF 轉成百分比\n",
    "    \n",
    "    keyword_list=[]\n",
    "    synonyms_list=[]\n",
    "    \n",
    "    def filter_by_tag(word):\n",
    "        v_tag = ['v', 'vd', 'vg', 'vi', 'vn', 'vq', 'vt']\n",
    "        n_tag = ['n', 'ng', 'nr', 'nrfg', 'nrt', 'ns', 'nt','nz']\n",
    "        tag = segmentor.pseg_lcut(word)\n",
    "#         print tag['word'][0] + '(' + tag['tag'][0] + ')'\n",
    "        if tag['tag'][0] in v_tag or tag['tag'][0] in n_tag:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    for index, row in keyword_df.iterrows():\n",
    "\n",
    "        if filter_by_tag(row['word']):\n",
    "            keyword_list.append(row['word'])\n",
    "        \n",
    "#             w2v_df = w2v.predict(row['word'], num=5, min_confidence=0.5)\n",
    "#             for index, row in w2v_df.iterrows():\n",
    "#                 if filter_by_tag(row['word']):\n",
    "#                     synonyms_list.append(row['word'])\n",
    "        \n",
    "    print 'keyword:'\n",
    "    for i,k in enumerate(keyword_list):\n",
    "        \n",
    "        \n",
    "        print str(i+1) + '. ' + k\n",
    "\n",
    "#     print 'synonyms ： '\n",
    "#     for s in synonyms_list:\n",
    "#         print s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = #where\n",
      "target = 台北\n",
      "keyword:\n",
      "1. 大屯山\n",
      "2. 台北\n",
      "3. 中心\n"
     ]
    }
   ],
   "source": [
    "chat('台北大屯山托嬰中心在哪裡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label = #other\n",
      "target = \n",
      "keyword:\n",
      "1. 落建\n",
      "2. 洗髮乳\n",
      "3. 效果\n"
     ]
    }
   ],
   "source": [
    "chat('落建洗髮乳有效果嗎')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
