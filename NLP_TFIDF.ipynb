{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import *\n",
    "from gensim.models import TfidfModel\n",
    "from datetime import datetime\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba_stopwords_path = \"/home/charles/dataset/jieba/stopwords.txt\"\n",
    "\n",
    "wiki_texts_txt_zh = \"/home/charles/dataset/wiki/wiki_zh_tw.txt\"\n",
    "wiki_seg_txt = \"/home/charles/dataset/wiki/wiki_seg_tw.txt\"\n",
    "\n",
    "tfidf_model_path = \"/home/charles/dataset/tfidf_model/v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open(jieba_stopwords_path,'r') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分次load到memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dictionary(wiki_seg_txt, jieba_stopwords_path):\n",
    "    \n",
    "#     start_time = datetime.now()\n",
    "    \n",
    "#     dictionary = Dictionary()\n",
    "#     corpus_orgin = []\n",
    "#     count = 0\n",
    "    \n",
    "#     corpus_list = []\n",
    "#     with open(wiki_seg_txt, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         print(lines.__len__())\n",
    "        \n",
    "#         for sentence in lines:\n",
    "#             words = sentence.split(\" \")\n",
    "#             sentence_segment = []\n",
    "            \n",
    "#             for word in words:           \n",
    "#                 if word.strip() != '':\n",
    "#                     if word.strip() not in stopword_set:\n",
    "#                         sentence_segment.append(word.strip())\n",
    "                        \n",
    "#             corpus_list.append(sentence_segment)\n",
    "\n",
    "#             count += 1\n",
    "#             if count % 25000 == 0:\n",
    "#                 print(\"getCorpus log : \" + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#                 print(\"processd \" + str(count) + \" segment_sentence\")\n",
    "#                 dictionary.add_documents(corpus_list)\n",
    "#                 corpus_list = []\n",
    "\n",
    "# #             if count % 200000 == 0:\n",
    "# #                 break\n",
    "    \n",
    "#     end_time = datetime.now()\n",
    "#     print(end_time-start_time)\n",
    "# #     return corpus_list\n",
    "#     return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329589\n",
      "getCorpus log : 2019-01-28 09:42:24\n",
      "processd 25000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:44:14\n",
      "processd 50000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:45:37\n",
      "processd 75000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:46:54\n",
      "processd 100000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:48:03\n",
      "processd 125000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:49:05\n",
      "processd 150000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:50:50\n",
      "processd 175000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:52:44\n",
      "processd 200000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:54:48\n",
      "processd 225000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:56:46\n",
      "processd 250000 segment_sentence\n",
      "getCorpus log : 2019-01-28 09:58:47\n",
      "processd 275000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:00:41\n",
      "processd 300000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:02:53\n",
      "processd 325000 segment_sentence\n",
      "0:22:51.880691\n"
     ]
    }
   ],
   "source": [
    "# dictionary = get_dictionary(wiki_seg_txt, jieba_stopwords_path)\n",
    "# # corpus_list = getCorpus(wiki_seg_txt, jieba_stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GetCorpus(object):\n",
    "#     def __iter__(self):\n",
    "        \n",
    "#         with open(wiki_seg_txt, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "#             print(lines.__len__())\n",
    "#             for sentence in lines:\n",
    "#                 words = sentence.split(\" \")\n",
    "#                 sentence_segment = []\n",
    "#                 corpus_list = []\n",
    "\n",
    "#                 for word in words:           \n",
    "#                     if word.strip() != '':\n",
    "#                         if word.strip() not in stopword_set:\n",
    "#                             yield dictionary.doc2bow(word.strip())     \n",
    "# #         for line in open(wiki_seg_txt):\n",
    "# #             yield dictionary.doc2bow(line.lower().split(\" \"))\n",
    "\n",
    "# corpus = GetCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "# MmCorpus.serialize(tfidf_model_path+\"/tfidf_corpuse.mm\", corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一次load到memory(至少16G training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorpus(wiki_seg_txt):\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    corpus_orgin = []\n",
    "    count = 0\n",
    "    corpus_list = []\n",
    "    with open(wiki_seg_txt, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(lines.__len__())\n",
    "        for sentence in lines:\n",
    "            words = sentence.split(\" \")\n",
    "            sentence_segment = []\n",
    "            for word in words:\n",
    "                if word.strip() != '':\n",
    "                    if word.strip() not in stopword_set:\n",
    "                        sentence_segment.append(word.strip())\n",
    "            corpus_list.append(sentence_segment)\n",
    "            count += 1\n",
    "            if count % 25000 == 0:\n",
    "                print(\"getCorpus log : \" + datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                print(\"processd \" + str(count) + \" segment_sentence\")\n",
    "                \n",
    "    end_time = datetime.now()\n",
    "    print(end_time-start_time)\n",
    "    return corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329589\n",
      "getCorpus log : 2019-01-28 10:22:30\n",
      "processd 25000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:22:48\n",
      "processd 50000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:23:04\n",
      "processd 75000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:23:21\n",
      "processd 100000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:23:39\n",
      "processd 125000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:23:54\n",
      "processd 150000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:24:09\n",
      "processd 175000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:24:23\n",
      "processd 200000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:24:43\n",
      "processd 225000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:24:57\n",
      "processd 250000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:25:10\n",
      "processd 275000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:25:31\n",
      "processd 300000 segment_sentence\n",
      "getCorpus log : 2019-01-28 10:25:42\n",
      "processd 325000 segment_sentence\n",
      "0:03:41.182206\n"
     ]
    }
   ],
   "source": [
    "corpus_list = getCorpus(wiki_seg_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start build dictionary\n",
      "0:16:00.671366\n",
      "end build dictionary\n"
     ]
    }
   ],
   "source": [
    "id2word = {}\n",
    "\n",
    "# 生成并保存字典\n",
    "print 'start build dictionary'\n",
    "start_time = datetime.now()\n",
    "\n",
    "dictionary = Dictionary(corpus_list)\n",
    "dictionary.save(tfidf_model_path+\"/tfidf_corpus_dict\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(end_time-start_time)\n",
    "print 'end build dictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start build corpuse.mm\n",
      "1:09:36.407724\n",
      "end build corpuse.mm\n"
     ]
    }
   ],
   "source": [
    "# 将文档转换成词袋(bag of words)模型\n",
    "\n",
    "print 'start build corpuse.mm'\n",
    "start_time = datetime.now()\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in corpus_list]\n",
    "\n",
    "# 保存生成的语料\n",
    "MmCorpus.serialize(tfidf_model_path+\"/tfidf_corpuse.mm\", corpus)\n",
    "\n",
    "del corpus_list\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(end_time-start_time)\n",
    "print 'end build corpuse.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.tfidfmodel:constructor received both corpus and explicit inverse document frequencies; ignoring the corpus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traing model \n",
      "0:00:53.447527\n",
      "end traing model \n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    print 'start traing model '\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    id2word = {}\n",
    "    tfidfModel = TfidfModel(corpus=corpus, id2word=id2word, dictionary=dictionary)\n",
    "    tfidfModel.save(tfidf_model_path+\"/tfidf.model\")\n",
    "#     corpus_tfidf = tfidfModel[corpus]\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    print(end_time-start_time)\n",
    "    print 'end traing model '\n",
    "    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel.load(tfidf_model_path+\"/tfidf.model\")\n",
    "# tfidf_model = TfidfModel.load(\"/home/charles/dataset/tfidf_model/tfidf.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.6044769270741118), (1, 0.796622648833837)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model[[(0,1),(1,1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/charles/dataset/jieba/dict_taiwan.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /home/charles/dataset/jieba/dict_taiwan.txt ...\n",
      "Loading model from cache /tmp/jieba.u48306fa201322dcccc3d0c62898fbadc.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.u48306fa201322dcccc3d0c62898fbadc.cache\n",
      "Loading model cost 0.508 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.508 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "jieba_dict_path1 = \"/home/charles/dataset/jieba/dict_taiwan.txt\"\n",
    "jieba_dict_path2 = \"/home/charles/dataset/jieba/userdict.txt\"\n",
    "jieba_dict_path3 = \"/home/charles/dataset/jieba/dict.txt.big\"\n",
    "jieba_dict_path4 = \"/home/charles/dataset/jieba/dict.txt.small\"\n",
    "\n",
    "# jieba custom setting.\n",
    "jieba.set_dictionary(jieba_dict_path1)\n",
    "jieba.load_userdict(jieba_dict_path2)\n",
    "jieba.load_userdict(jieba_dict_path3)\n",
    "jieba.load_userdict(jieba_dict_path4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary.load(tfidf_model_path+\"/tfidf_corpus_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "陳水扁\n",
      "貪污\n",
      "收黑\n",
      "錢\n"
     ]
    }
   ],
   "source": [
    "# jieba分词\n",
    "text = u'陳水扁貪污收黑錢'\n",
    "test_cut_raw_0 = jieba.lcut(text)\n",
    "for w in test_cut_raw_0:\n",
    "    print w\n",
    "#     print type(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba分词语料集 去除停用词\n",
    "corpus0 = []\n",
    "\n",
    "for word in test_cut_raw_0:\n",
    "    if word.strip() != '':\n",
    "        if word not in stopword_set:\n",
    "            corpus0.append(word)\n",
    "\n",
    "for word in corpus0:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus_0 = dictionary.doc2bow(corpus0)\n",
    "test_corpus_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus_tfidf_0 = tfidf_model[test_corpus_0]\n",
    "test_corpus_tfidf_0 = sorted(test_corpus_tfidf_0, key=lambda item: item[1], reverse=True)\n",
    "test_corpus_tfidf_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2token = dict(zip(dictionary.token2id.values(), dictionary.token2id.keys()))\n",
    "result = []\n",
    "for i in range(len(test_corpus_tfidf_0)):\n",
    "    result.append({id2token[test_corpus_tfidf_0[i][0]]: test_corpus_tfidf_0[i][1]})\n",
    "    print id2token[test_corpus_tfidf_0[i][0]] + ' : ' + str(test_corpus_tfidf_0[i][1])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_w2v_model(w2v_model_path):\n",
    "    return models.Word2Vec.load(w2v_model_path)\n",
    "\n",
    "def w2v_predict(model, input_text):\n",
    "#     print \"--------------------------------------\"\n",
    "    print 'synonyms : '\n",
    "    res = model.wv.most_similar(input_text)\n",
    "    for index,item in enumerate(res):\n",
    "        if index == 5:\n",
    "            break\n",
    "        \n",
    "        if item[1] < 0.15:\n",
    "            break\n",
    "        \n",
    "        cut = pseg.lcut(item[0])       \n",
    "        print item[0] + \"(\" + list(cut[0])[1] + \") :\" + str(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_path = \"/home/charles/dataset/word2vec_model/v2/word2vec_tw.model\"\n",
    "w2v_model = load_w2v_model(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請問 (v)\n",
      "請問 (v)\n",
      "今天 (t)\n",
      "下雨 (v)\n",
      "嗎 (y)\n",
      "---subject_dict---\n",
      "下雨 (v)\n",
      "請問 (v)\n",
      "今天 (t)\n",
      "嗎 (y)\n"
     ]
    }
   ],
   "source": [
    "test_cut =pseg.lcut('請問請問今天下雨嗎')\n",
    "text_subject_dict={}\n",
    "for w,n in test_cut:\n",
    "    print w + ' (' + n + ')'\n",
    "    text_subject_dict.update({w:n})\n",
    "print '---subject_dict---'\n",
    "for k, v in text_subject_dict.items():\n",
    "    print k + ' (' + v + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "給/p\n",
      "我/r\n",
      "你/r\n",
      "的/uj\n",
      "臉書/n\n"
     ]
    }
   ],
   "source": [
    "test_cut =pseg.lcut('給我你的臉書')\n",
    "for w in test_cut:\n",
    " print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "給我\n",
      "你\n",
      "的\n",
      "臉書\n"
     ]
    }
   ],
   "source": [
    "test_cut_raw_0 = jieba.lcut('給我你的臉書')\n",
    "for w in test_cut_raw_0:\n",
    " print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keyword(text):\n",
    "#     test_cut_raw_0 = jieba.lcut(text)\n",
    "    test_cut_raw_0 =pseg.lcut(text)\n",
    "    text_subject_dict={}\n",
    "    for w,n in test_cut_raw_0:\n",
    "        print w + ' (' + n + ')'\n",
    "        text_subject_dict.update({w:n})\n",
    "    \n",
    "    # jieba分词语料集 去除停用词\n",
    "    corpus0 = []\n",
    "#     # load stopwords set\n",
    "#     stopword_set = set()\n",
    "#     with open(jieba_stopwords_path,'r') as stopwords:\n",
    "#         for stopword in stopwords:\n",
    "#             stopword_set.add(stopword.strip('\\n').decode('utf8'))\n",
    "    for w,n in test_cut_raw_0:\n",
    "        if w not in stopword_set:\n",
    "            corpus0.append(w)\n",
    "    test_corpus_0 = dictionary.doc2bow(corpus0)\n",
    "    test_corpus_tfidf_0 = tfidf_model[test_corpus_0]\n",
    "    test_corpus_tfidf_0 = sorted(test_corpus_tfidf_0, key=lambda item: item[1], reverse=True)\n",
    "    id2token = dict(zip(dictionary.token2id.values(), dictionary.token2id.keys()))\n",
    "    result = []\n",
    "    for i in range(len(test_corpus_tfidf_0)):\n",
    "        result.append({id2token[test_corpus_tfidf_0[i][0]]: test_corpus_tfidf_0[i][1]})\n",
    "        print \"--------------------------------------\"\n",
    "        word = id2token[test_corpus_tfidf_0[i][0]]\n",
    "        tfidf = test_corpus_tfidf_0[i][1]\n",
    "        print 'keyword: ' + word + '(' + text_subject_dict[word] + ') , tfidf: ' + str(tfidf)\n",
    "        try:\n",
    "            w2v_predict(w2v_model, word)\n",
    "        except KeyError as er:\n",
    "            print 'no synonyms'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "給 (p)\n",
      "我 (r)\n",
      "你 (r)\n",
      "的 (uj)\n",
      "臉書 (n)\n",
      "--------------------------------------\n",
      "keyword: 臉書(n) , tfidf: 1.0\n",
      "synonyms : \n",
      "facebook(eng) :0.750346064568\n",
      "粉絲團(n) :0.71051210165\n",
      "貼文(x) :0.703195214272\n",
      "發文(n) :0.688820421696\n",
      "推特(x) :0.676334142685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "find_keyword('給我你的臉書')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "關雲長 (nr)\n",
      "是 (v)\n",
      "那個 (r)\n",
      "時代 (n)\n",
      "的 (uj)\n",
      "人 (n)\n",
      "--------------------------------------\n",
      "keyword: 關雲長(nr) , tfidf: 0.945486674207714\n",
      "synonyms : \n",
      "三氣(m) :0.821100533009\n",
      "楊六郎(nr) :0.795885562897\n",
      "鴻門宴(n) :0.766659557819\n",
      "修慶(nr) :0.758520543575\n",
      "蔡文姬(nr) :0.754626989365\n",
      "--------------------------------------\n",
      "keyword: 時代(n) , tfidf: 0.30168591923575705\n",
      "synonyms : \n",
      "時期(n) :0.701734364033\n",
      "黃金時代(t) :0.493782132864\n",
      "戰國時代(n) :0.476719617844\n",
      "幕府時代(n) :0.465163141489\n",
      "末期(f) :0.442668646574\n",
      "--------------------------------------\n",
      "keyword: 人(n) , tfidf: 0.12263994060057355\n",
      "synonyms : \n",
      "人會(n) :0.60105741024\n",
      "人則(n) :0.60071003437\n",
      "人並(n) :0.592273414135\n",
      "人為(n) :0.588895022869\n",
      "人有(n) :0.5797290802\n"
     ]
    }
   ],
   "source": [
    "find_keyword('關雲長是那個時代的人')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "請問 (v)\n",
      "今天 (t)\n",
      "下雨 (v)\n",
      "嗎 (y)\n",
      "？ (x)\n",
      "--------------------------------------\n",
      "keyword: 請問(v) , tfidf: 0.6941697454757373\n",
      "synonyms : \n",
      "來點(n) :0.718818247318\n",
      "有話要說(l) :0.565844118595\n",
      "想點(v) :0.558074116707\n",
      "說了算(l) :0.529360890388\n",
      "幹嘛(n) :0.524327337742\n",
      "--------------------------------------\n",
      "keyword: 下雨(v) , tfidf: 0.6091738213962883\n",
      "synonyms : \n",
      "下雪(v) :0.790796756744\n",
      "大雨(n) :0.678220510483\n",
      "放晴(v) :0.668708503246\n",
      "天色(n) :0.655068993568\n",
      "雨天(t) :0.638269841671\n",
      "--------------------------------------\n",
      "keyword: 今天(t) , tfidf: 0.38345223925750294\n",
      "synonyms : \n",
      "現在(t) :0.698509812355\n",
      "今日(t) :0.670372664928\n",
      "如今(t) :0.622143864632\n",
      "那時(r) :0.542845964432\n",
      "現今(t) :0.530251502991\n"
     ]
    }
   ],
   "source": [
    "find_keyword('請問今天下雨嗎？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "射手座 (n)\n",
      "的 (uj)\n",
      "女生 (n)\n",
      "活潑開朗 (z)\n",
      "嗎 (y)\n",
      "--------------------------------------\n",
      "keyword: 活潑開朗(z) , tfidf: 0.6532851468133041\n",
      "synonyms : \n",
      "天真爛漫(i) :0.873540043831\n",
      "乖巧(a) :0.848528862\n",
      "活潑可愛(l) :0.848128259182\n",
      "爽朗(a) :0.845764100552\n",
      "好強(n) :0.8444404006\n",
      "--------------------------------------\n",
      "keyword: 射手座(n) , tfidf: 0.627669598665805\n",
      "synonyms : \n",
      "處女座(n) :0.845364689827\n",
      "天秤座(n) :0.811121344566\n",
      "牡羊座(n) :0.809569716454\n",
      "魔羯座(nz) :0.801879286766\n",
      "雙魚座(nz) :0.798948287964\n",
      "--------------------------------------\n",
      "keyword: 女生(n) , tfidf: 0.4233785444065713\n",
      "synonyms : \n",
      "男生(n) :0.877353847027\n",
      "女同學(n) :0.694101929665\n",
      "女孩子(n) :0.672837495804\n",
      "男同學(n) :0.666989684105\n",
      "小學生(nr) :0.643523573875\n"
     ]
    }
   ],
   "source": [
    "find_keyword('射手座的女生活潑開朗嗎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "歐幾 (n)\n",
      "裡 (f)\n",
      "得 (ud)\n",
      "西元前 (t)\n",
      "三 (m)\n",
      "世紀 (n)\n",
      "的 (uj)\n",
      "古希臘 (ns)\n",
      "數學家 (n)\n",
      "--------------------------------------\n",
      "keyword: 歐幾(n) , tfidf: 0.6063313241952766\n",
      "synonyms : \n",
      "裡得(f) :0.898319005966\n",
      "歐幾里得(ns) :0.851940989494\n",
      "歐幾里德(ns) :0.800018429756\n",
      "仿射(v) :0.748724579811\n",
      "旋量(n) :0.745209217072\n",
      "--------------------------------------\n",
      "keyword: 西元前(t) , tfidf: 0.4877554803017211\n",
      "synonyms : \n",
      "公元前(t) :0.884474992752\n",
      "公元(q) :0.78594148159\n",
      "六世紀(t) :0.752815902233\n",
      "十一世紀(x) :0.687458753586\n",
      "距今(n) :0.68706703186\n",
      "--------------------------------------\n",
      "keyword: 古希臘(ns) , tfidf: 0.4131048555777275\n",
      "synonyms : \n",
      "古羅馬(ns) :0.749866843224\n",
      "古埃及(ns) :0.713053762913\n",
      "古希臘人(x) :0.705481767654\n",
      "希臘(ns) :0.671430528164\n",
      "希臘語(nz) :0.670438408852\n",
      "--------------------------------------\n",
      "keyword: 數學家(n) , tfidf: 0.39429620805022375\n",
      "synonyms : \n",
      "物理學家(n) :0.770481228828\n",
      "邏輯學家(n) :0.762344658375\n",
      "幾何學家(l) :0.739117205143\n",
      "天文學家(n) :0.720733761787\n",
      "哲學家(n) :0.68972748518\n",
      "--------------------------------------\n",
      "keyword: 世紀(n) , tfidf: 0.18997541662769218\n",
      "synonyms : \n",
      "十九世紀(nz) :0.859842061996\n",
      "年代(t) :0.797096550465\n",
      "十八世紀(nz) :0.789299547672\n",
      "二十世紀(nz) :0.778233289719\n",
      "十七世紀(nz) :0.773076355457\n",
      "--------------------------------------\n",
      "keyword: 裡(f) , tfidf: 0.17955817003883187\n",
      "synonyms : \n",
      "裡的(f) :0.62677192688\n",
      "裡與(f) :0.577531695366\n",
      "裡面(n) :0.558728396893\n",
      "中(f) :0.548389971256\n",
      "裡耶爾(nrt) :0.537928879261\n"
     ]
    }
   ],
   "source": [
    "find_keyword('歐幾裡得西元前三世紀的古希臘數學家')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以 (c)\n",
      "幫 (v)\n",
      "我 (r)\n",
      "打電話 (l)\n",
      "給 (p)\n",
      "陳俊宏 (x)\n",
      "嗎 (y)\n",
      "--------------------------------------\n",
      "keyword: 陳俊宏(x) , tfidf: 0.8402577744492467\n",
      "synonyms : \n",
      "楊西昆(nr) :0.744835615158\n",
      "洪冬桂(x) :0.742252469063\n",
      "曹聖芬(nr) :0.739722847939\n",
      "楊繼增(nr) :0.735281646252\n",
      "錦芳(j) :0.735161364079\n",
      "--------------------------------------\n",
      "keyword: 打電話(l) , tfidf: 0.5421871194316579\n",
      "synonyms : \n",
      "寄信(v) :0.72881513834\n",
      "打來(v) :0.700615644455\n",
      "來電(v) :0.684566617012\n",
      "致電(v) :0.666083812714\n",
      "發短信(j) :0.659898281097\n"
     ]
    }
   ],
   "source": [
    "find_keyword('可以幫我打電話給陳俊宏嗎')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "星巴克 (nr)\n",
      "買一送一 (l)\n",
      "實在 (v)\n",
      "是 (v)\n",
      "太 (d)\n",
      "划算 (v)\n",
      "啦 (y)\n",
      ", (x)\n",
      "  (x)\n",
      "你 (r)\n",
      "可以 (c)\n",
      "幫 (v)\n",
      "我 (r)\n",
      "買杯 (x)\n",
      "咖啡 (n)\n",
      "嗎 (y)\n",
      "？ (x)\n",
      "--------------------------------------\n",
      "keyword: 買一送一(l) , tfidf: 0.575571831371829\n",
      "synonyms : \n",
      "改潮(v) :0.63635802269\n",
      "肥滋滋(a) :0.629618942738\n",
      "mascara(eng) :0.628269910812\n",
      "upci(eng) :0.627392828465\n",
      "眼紅紅(n) :0.623952925205\n",
      "--------------------------------------\n",
      "keyword: 划算(v) , tfidf: 0.522456019767488\n",
      "synonyms : \n",
      "搶手(v) :0.727142930031\n",
      "便宜(a) :0.706758737564\n",
      "有利可圖(l) :0.670373857021\n",
      "值錢(v) :0.644770443439\n",
      "吃香(v) :0.643182039261\n",
      "--------------------------------------\n",
      "keyword: 星巴克(nr) , tfidf: 0.4214651979522445\n",
      "synonyms : \n",
      "starbucks(eng) :0.802739024162\n",
      "連鎖店(n) :0.802303135395\n",
      "肯德基(nr) :0.792109131813\n",
      "專賣店(n) :0.776521146297\n",
      "必勝客(l) :0.758501768112\n",
      "--------------------------------------\n",
      "keyword: 實在(v) , tfidf: 0.31794900976469115\n",
      "synonyms : \n",
      "簡直(d) :0.758505821228\n",
      "畢竟(d) :0.733420848846\n",
      "的確(d) :0.724726200104\n",
      "這都(r) :0.709084510803\n",
      "顯然(ad) :0.690440416336\n",
      "--------------------------------------\n",
      "keyword: 咖啡(n) , tfidf: 0.29045387940140593\n",
      "synonyms : \n",
      "紅茶(nr) :0.699404120445\n",
      "果汁(n) :0.682089149952\n",
      "冰淇淋(nr) :0.64737033844\n",
      "牛奶(n) :0.640094518661\n",
      "飲品(n) :0.622570872307\n",
      "--------------------------------------\n",
      "keyword: 太(d) , tfidf: 0.18074521400411708\n",
      "synonyms : \n",
      "實在太(n) :0.626541435719\n",
      "過於(v) :0.604874968529\n",
      "太及(ns) :0.546764791012\n",
      "太過(d) :0.478154242039\n",
      "非常(d) :0.473066329956\n"
     ]
    }
   ],
   "source": [
    "find_keyword('星巴克買一送一實在是太划算啦, 你可以幫我買杯咖啡嗎？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "林百里 (nr)\n",
      "很 (d)\n",
      "壞 (a)\n",
      "一直 (d)\n",
      "要 (v)\n",
      "我 (r)\n",
      "加班 (v)\n",
      "--------------------------------------\n",
      "keyword: 林百里(nr) , tfidf: 0.7277598544254239\n",
      "synonyms : \n",
      "張忠謀(x) :0.693802118301\n",
      "施振榮(x) :0.657089650631\n",
      "梁次震(x) :0.657002568245\n",
      "曹興誠(x) :0.646898031235\n",
      "英業達(nt) :0.621202588081\n",
      "--------------------------------------\n",
      "keyword: 加班(v) , tfidf: 0.5530642897081026\n",
      "synonyms : \n",
      "加班費(n) :0.750143826008\n",
      "輪班(n) :0.639718294144\n",
      "加薪(nr) :0.625440299511\n",
      "補休(v) :0.623734295368\n",
      "上班(v) :0.620047569275\n",
      "--------------------------------------\n",
      "keyword: 壞(a) , tfidf: 0.40556810246413316\n",
      "synonyms : \n",
      "要命(v) :0.52674382925\n",
      "蠢(a) :0.526574611664\n",
      "乖(a) :0.515872836113\n",
      "說謊(v) :0.510289907455\n",
      "貪心(v) :0.507982194424\n"
     ]
    }
   ],
   "source": [
    "find_keyword('林百里很壞一直要我加班')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李白 (nr)\n",
      "是 (v)\n",
      "哪個 (r)\n",
      "朝代 (n)\n",
      "的 (uj)\n",
      "人 (n)\n",
      "--------------------------------------\n",
      "keyword: 朝代(n) , tfidf: 0.7036467812454269\n",
      "synonyms : \n",
      "諸侯國(nr) :0.702491641045\n",
      "各朝(r) :0.640406012535\n",
      "周代(t) :0.607387065887\n",
      "五代十國(nz) :0.590109109879\n",
      "郡縣(ns) :0.580022573471\n",
      "--------------------------------------\n",
      "keyword: 李白(nr) , tfidf: 0.7004124458643536\n",
      "synonyms : \n",
      "杜甫(nr) :0.817996501923\n",
      "孟浩然(nr) :0.784346103668\n",
      "劉禹錫(nr) :0.767086625099\n",
      "白居易(nr) :0.764298856258\n",
      "有詩(v) :0.757497191429\n",
      "--------------------------------------\n",
      "keyword: 人(n) , tfidf: 0.11959771285967173\n",
      "synonyms : \n",
      "人會(n) :0.60105741024\n",
      "人則(n) :0.60071003437\n",
      "人並(n) :0.592273414135\n",
      "人為(n) :0.588895022869\n",
      "人有(n) :0.5797290802\n"
     ]
    }
   ],
   "source": [
    "find_keyword('李白是哪個朝代的人')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高速公路 (n)\n",
      "塞車 (ns)\n",
      "嗎 (y)\n",
      "？ (x)\n",
      "--------------------------------------\n",
      "keyword: 塞車(ns) , tfidf: 0.8571111928169417\n",
      "synonyms : \n",
      "堵車(v) :0.715826392174\n",
      "交通堵塞(n) :0.690776288509\n",
      "擠塞(v) :0.690232634544\n",
      "擁堵(v) :0.684286475182\n",
      "壅塞(v) :0.674050629139\n",
      "--------------------------------------\n",
      "keyword: 高速公路(n) , tfidf: 0.5151314425929748\n",
      "synonyms : \n",
      "省道(n) :0.733386456966\n",
      "公路(n) :0.732202589512\n",
      "高速路(n) :0.670711040497\n",
      "收費公路(n) :0.665576934814\n",
      "國道(n) :0.665448248386\n"
     ]
    }
   ],
   "source": [
    "find_keyword('高速公路塞車嗎？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "臺中 (s)\n",
      "花博 (x)\n",
      "公園 (n)\n",
      "好多 (m)\n",
      "人 (n)\n",
      "--------------------------------------\n",
      "keyword: 花博(x) , tfidf: 0.6442249300363727\n",
      "synonyms : \n",
      "流行館(x) :0.682482004166\n",
      "艷館(x) :0.656282663345\n",
      "中正紀念堂(n) :0.629564642906\n",
      "花博會(nr) :0.62250494957\n",
      "燈會(n) :0.616178035736\n",
      "--------------------------------------\n",
      "keyword: 好多(m) , tfidf: 0.6151680344024876\n",
      "synonyms : \n",
      "這麼多(x) :0.737200915813\n",
      "那麼多(x) :0.708552479744\n",
      "啥(r) :0.689442396164\n",
      "嘛(y) :0.647598981857\n",
      "一大堆(m) :0.642938494682\n",
      "--------------------------------------\n",
      "keyword: 臺中(s) , tfidf: 0.3579877739448746\n",
      "synonyms : \n",
      "臺南(ns) :0.721824705601\n",
      "彰化(v) :0.691546082497\n",
      "臺中市(ns) :0.691190719604\n",
      "高雄(nr) :0.679966807365\n",
      "新竹(ns) :0.661275148392\n",
      "--------------------------------------\n",
      "keyword: 公園(n) , tfidf: 0.2643364480864175\n",
      "synonyms : \n",
      "公園站(x) :0.668405652046\n",
      "保護區(n) :0.598857939243\n",
      "森林公園(n) :0.598241209984\n",
      "園內(s) :0.579810976982\n",
      "高爾夫球場(n) :0.574180841446\n",
      "--------------------------------------\n",
      "keyword: 人(n) , tfidf: 0.09226876442253198\n",
      "synonyms : \n",
      "人會(n) :0.60105741024\n",
      "人則(n) :0.60071003437\n",
      "人並(n) :0.592273414135\n",
      "人為(n) :0.588895022869\n",
      "人有(n) :0.5797290802\n"
     ]
    }
   ],
   "source": [
    "find_keyword('臺中花博公園好多人')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
