{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki詞庫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "維基百科下載好後，先別急著解壓縮，因為這是一份 xml 文件，裏頭佈滿了各式各樣的標籤，我們得先想辦法送走這群不速之客，不過也別太擔心，gensim 早已看穿了一切，藉由調用 wikiCorpus，我們能很輕鬆的只取出文章的標題和內容。\n",
    "\n",
    "初始化WikiCorpus後，能藉由get_texts()可迭代每一篇文章，它所回傳的是一個tokens list，我以空白符將這些 tokens 串接起來，統一輸出到同一份文字檔裡。這邊要注意一件事，get_texts()受wikicorpus.py中的變數ARTICLE_MIN_WORDS限制，只會回傳內容長度大於 50 的文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from gensim.corpora import WikiCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_corpus(wiki_data_path, wiki_texts_txt):\n",
    "\n",
    "#     if len(sys.argv) != 2:\n",
    "#         print(\"Usage: python3 \" + sys.argv[0] + \" wiki_data_path\")\n",
    "#         exit()\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    wiki_corpus = WikiCorpus(wiki_data_path, dictionary={})\n",
    "    texts_num = 0\n",
    "\n",
    "    with open(wiki_texts_txt,'w') as output:\n",
    "        for text in wiki_corpus.get_texts():\n",
    "#             text = text.decode('utf-8')\n",
    "            output.write(' '.join(text).encode('utf-8') + '\\n')\n",
    "            texts_num += 1\n",
    "            if texts_num % 10000 == 0:\n",
    "                print(\"已處理\" + texts_num + \"篇文章\")\n",
    "\n",
    "wiki_data_path = \"/home/charles/dataset/zhwiki-latest-pages-articles.xml.bz2\"\n",
    "wiki_texts_txt = \"/home/charles/dataset/wiki_texts.txt\"\n",
    "# wiki_corpus(wiki_data_path, wiki_texts_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(jieba_dict_path, jieba_dict_append_path, jieba_stopwords_path,\n",
    "            wiki_texts_txt_zh, wiki_seg_txt):\n",
    "\n",
    "#     logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    start = datetime.now()\n",
    "    \n",
    "    # jieba custom setting.\n",
    "    jieba.set_dictionary(jieba_dict_path)\n",
    "    for path in jieba_dict_append_path:\n",
    "        print path\n",
    "        jieba.load_userdict(path)\n",
    "\n",
    "    # load stopwords set\n",
    "    stopword_set = set()\n",
    "    with open(jieba_stopwords_path,'r') as stopwords:\n",
    "        for stopword in stopwords:\n",
    "            stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "    output = open(wiki_seg_txt, 'w')\n",
    "    with open(wiki_texts_txt_zh, 'r') as content :\n",
    "        for texts_num, line in enumerate(content):\n",
    "            line = line.strip('\\n')\n",
    "            words = jieba.cut(line, cut_all=False)\n",
    "            for word in words:\n",
    "                if word not in stopword_set:\n",
    "                    output.write(word.encode('utf-8') + ' ')\n",
    "            output.write('\\n')\n",
    "\n",
    "            if (texts_num + 1) % 10000 == 0:\n",
    "                print(\"已完成前 \" + str(texts_num + 1) + \" 行的斷詞\")\n",
    "\n",
    "    output.close()\n",
    "    \n",
    "    end = datetime.now()\n",
    "    segment_duration = str(end - start)\n",
    "    print 'segment_duration : ' + segment_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /home/charles/dataset/jieba/dict_taiwan.txt ...\n",
      "DEBUG:jieba:Building prefix dict from /home/charles/dataset/jieba/dict_taiwan.txt ...\n",
      "Loading model from cache /tmp/jieba.u48306fa201322dcccc3d0c62898fbadc.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/jieba.u48306fa201322dcccc3d0c62898fbadc.cache\n",
      "Loading model cost 0.698 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.698 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/charles/dataset/jieba/userdict.txt\n",
      "/home/charles/dataset/jieba/dict.txt.big\n",
      "/home/charles/dataset/jieba/dict.txt.small\n",
      "已完成前 10000 行的斷詞\n",
      "已完成前 20000 行的斷詞\n",
      "已完成前 30000 行的斷詞\n",
      "已完成前 40000 行的斷詞\n",
      "已完成前 50000 行的斷詞\n",
      "已完成前 60000 行的斷詞\n",
      "已完成前 70000 行的斷詞\n",
      "已完成前 80000 行的斷詞\n",
      "已完成前 90000 行的斷詞\n",
      "已完成前 100000 行的斷詞\n",
      "已完成前 110000 行的斷詞\n",
      "已完成前 120000 行的斷詞\n",
      "已完成前 130000 行的斷詞\n",
      "已完成前 140000 行的斷詞\n",
      "已完成前 150000 行的斷詞\n",
      "已完成前 160000 行的斷詞\n",
      "已完成前 170000 行的斷詞\n",
      "已完成前 180000 行的斷詞\n",
      "已完成前 190000 行的斷詞\n",
      "已完成前 200000 行的斷詞\n",
      "已完成前 210000 行的斷詞\n",
      "已完成前 220000 行的斷詞\n",
      "已完成前 230000 行的斷詞\n",
      "已完成前 240000 行的斷詞\n",
      "已完成前 250000 行的斷詞\n",
      "已完成前 260000 行的斷詞\n",
      "已完成前 270000 行的斷詞\n",
      "已完成前 280000 行的斷詞\n",
      "已完成前 290000 行的斷詞\n",
      "已完成前 300000 行的斷詞\n",
      "已完成前 310000 行的斷詞\n",
      "已完成前 320000 行的斷詞\n",
      "segment_duration : 1:42:06.482440\n"
     ]
    }
   ],
   "source": [
    "jieba_stopwords_path = \"/home/charles/dataset/jieba/stopwords.txt\"\n",
    "jieba_dict_path1 = \"/home/charles/dataset/jieba/dict_taiwan.txt\"\n",
    "jieba_dict_path2 = \"/home/charles/dataset/jieba/userdict.txt\"\n",
    "jieba_dict_path3 = \"/home/charles/dataset/jieba/dict.txt.big\"\n",
    "jieba_dict_path4 = \"/home/charles/dataset/jieba/dict.txt.small\"\n",
    "\n",
    "wiki_texts_txt_zh = \"/home/charles/dataset/wiki/wiki_zh_tw.txt\"\n",
    "wiki_seg_txt_tw = \"/home/charles/dataset/wiki/wiki_seg_tw.txt\"\n",
    "\n",
    "# segment(jieba_dict_path1, [jieba_dict_path2, jieba_dict_path3, jieba_dict_path4],\n",
    "#         jieba_stopwords_path, wiki_texts_txt_zh, wiki_seg_txt_tw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class gensim.models.word2vec.Word2Vec(sentences=None, size=100, lpha=0.025, \n",
    "                                      window=5, min_count=5, max_vocab_size=None, sample=0.001, \n",
    "                                      seed=1, \n",
    "                                      workers=3, \n",
    "                                      min_alpha=0.0001, \n",
    "                                      sg=0, hs=0, \n",
    "                                      negative=5, cbow_mean=1, \n",
    "                                      hashfxn=<built-in function hash>, iter=5, \n",
    "                                      null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)\n",
    "\n",
    "sentences:當然了，這是要訓練的句子集，沒有他就不用跑了\n",
    "size:這表示的是訓練出的詞向量會有幾維\n",
    "alpha:機器學習中的學習率，這東西會逐漸收斂到 min_alpha\n",
    "sg:這個不是三言兩語能說完的，sg=1表示採用skip-gram,sg=0 表示採用cbow\n",
    "window:還記得孔乙己的例子嗎？能往左往右看幾個字的意思\n",
    "workers:執行緒數目，除非電腦不錯，不然建議別超過 4\n",
    "min_count:若這個詞出現的次數小於min_count，那他就不會被視為訓練對象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_duration : 0:57:00.178821\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "def train(wiki_seg_txt_tw, save_model_path):\n",
    "    \n",
    "    start = datetime.now()\n",
    "\n",
    "#     logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    sentences = word2vec.LineSentence(wiki_seg_txt_tw)\n",
    "    model = word2vec.Word2Vec(sentences, size=240)\n",
    "    #class gensim.models.word2vec.Word2Vec(sentences=None, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    model.save(save_model_path)\n",
    "    \n",
    "    end = datetime.now()\n",
    "    train_duration = str(end - start)\n",
    "    print 'train_duration : ' + train_duration=.p\n",
    "\n",
    "    #模型讀取方式\n",
    "    # model = word2vec.Word2Vec.load(\"your_model_name\")\n",
    "\n",
    "save_model_path = \"/home/charles/dataset/word2vec_model/v2/word2vec_tw.model\"\n",
    "# train(wiki_seg_txt_tw, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"/home/charles/dataset/word2vec_model/v2/word2vec_tw.model\"\n",
    "jieba_stopwords_path = \"/home/charles/dataset/jieba/stopwords.txt\"\n",
    "jieba_dict_path1 = \"/home/charles/dataset/jieba/dict_taiwan.txt\"\n",
    "jieba_dict_path2 = \"/home/charles/dataset/jieba/userdict.txt\"\n",
    "jieba_dict_path3 = \"/home/charles/dataset/jieba/dict.txt.big\"\n",
    "jieba_dict_path4 = \"/home/charles/dataset/jieba/dict.txt.small\"\n",
    "wiki_texts_txt_tw = \"/home/charles/dataset/wiki/wiki_zh_tw.txt\"\n",
    "wiki_seg_txt_tw = \"/home/charles/dataset/wiki/wiki_seg_tw.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "def load_model(save_model_path):\n",
    "    return models.Word2Vec.load(save_model_path)\n",
    "\n",
    "def predict(model, input_text):\n",
    "    print \"--------------------------------------\"\n",
    "    print 'input_text : ' + input_text\n",
    "    res = model.most_similar(input_text)\n",
    "    for item in res:\n",
    "#         if item[1] < 0.5:\n",
    "#             break\n",
    "        \n",
    "        print item[0] + \":\" + str(item[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 賴清德\n",
      "吳敦義:0.859070539474\n",
      "陳菊:0.850354075432\n",
      "柯文哲:0.84415268898\n",
      "柯建銘:0.836035966873\n",
      "郝龍斌:0.833289980888\n",
      "段宜康:0.833083331585\n",
      "遊錫堃:0.824838101864\n",
      "陳其邁:0.813775539398\n",
      "林全:0.81028431654\n",
      "謝長廷:0.802462458611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'賴清德')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 周董\n",
      "makiyo:0.618824958801\n",
      "丁小芹:0.617356181145\n",
      "小鍾:0.615242362022\n",
      "阿嬌:0.614605247974\n",
      "曹西平:0.608664512634\n",
      "錢君仲:0.607013165951\n",
      "艾成:0.60310614109\n",
      "李京恬:0.600842416286\n",
      "阿妹:0.600240468979\n",
      "黃立成:0.598941743374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'周董')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 機票\n",
      "船票:0.771833300591\n",
      "收據:0.697825551033\n",
      "發票:0.69179213047\n",
      "飛機票:0.686031401157\n",
      "火車票:0.667836308479\n",
      "付款:0.663617253304\n",
      "退款:0.660489857197\n",
      "手續費:0.656748473644\n",
      "旅費:0.651043117046\n",
      "服務費:0.6444003582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'機票')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 臺北\n",
      "高雄:0.690520644188\n",
      "臺北市:0.668921709061\n",
      "臺南:0.617259502411\n",
      "基隆:0.613428354263\n",
      "臺中:0.600836515427\n",
      "宜蘭:0.585610747337\n",
      "士林:0.566209793091\n",
      "新竹:0.557791113853\n",
      "桃園:0.552059412003\n",
      "中正紀念堂:0.548482358456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'臺北')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "/home/charles/anaconda2/lib/python2.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 總統府\n",
      "國防部:0.615797638893\n",
      "國安會:0.61290371418\n",
      "行政院長:0.611288189888\n",
      "立法院:0.603158831596\n",
      "外交部:0.59915626049\n",
      "國策顧問:0.5990254879\n",
      "邱義仁:0.597583174706\n",
      "行政院:0.595635294914\n",
      "監察院:0.591860711575\n",
      "郝柏村:0.591312408447\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'總統府')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 總統\n",
      "副總統:0.79043841362\n",
      "前總統:0.773817300797\n",
      "國防部長:0.692437648773\n",
      "總理:0.679863810539\n",
      "國務卿:0.650844752789\n",
      "代總統:0.640511453152\n",
      "民選總統:0.630449533463\n",
      "歐巴馬:0.630439758301\n",
      "馬杜羅:0.613203704357\n",
      "內政部長:0.601097941399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'總統')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 國防部長\n",
      "內政部長:0.843525052071\n",
      "外交部長:0.834659695625\n",
      "財政部長:0.79213309288\n",
      "司法部長:0.783093929291\n",
      "國務卿:0.76487839222\n",
      "前國防部長:0.751854538918\n",
      "交通部長:0.738543391228\n",
      "經濟部長:0.738034009933\n",
      "商務部長:0.736397027969\n",
      "代總統:0.73503446579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'國防部長')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 誤點\n",
      "晚點:0.69430989027\n",
      "超員:0.659165561199\n",
      "延誤:0.655866980553\n",
      "收班:0.633598923683\n",
      "超載:0.627989590168\n",
      "交通堵塞:0.622750639915\n",
      "多班:0.614429712296\n",
      "離站:0.6121134758\n",
      "故障:0.610724925995\n",
      "到站:0.603853702545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'誤點')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "input_text : 勞保\n",
      "保費:0.780389249325\n",
      "退休金:0.773757517338\n",
      "給付:0.770893931389\n",
      "勞退:0.753689169884\n",
      "國民年金:0.752222657204\n",
      "農保:0.749035477638\n",
      "請領:0.742509186268\n",
      "公保:0.740846931934\n",
      "資遣費:0.737330436707\n",
      "公教人員:0.720778524876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "predict(model, u'勞保')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
