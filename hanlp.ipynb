{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[This, wonderful, a]\n"
     ]
    }
   ],
   "source": [
    "text = 'This is a wonderful day '\n",
    "phraseList = HanLP.extractKeyword(text, 3)\n",
    "print(phraseList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "陈明忠\tnr\n",
      "下雨天\tn\n",
      "地面\tn\n",
      "积水\tn\n"
     ]
    }
   ],
   "source": [
    "for term in HanLP.segment('陈明忠下雨天地面积水'):\n",
    "    print('{}\\t{}'.format(term.word, term.nature)) # 获取单词与词性\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convertToTraditionalChinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用筆記本電腦寫程序\n",
      "“以后等你当上皇后，就能买草莓庆祝了”\n"
     ]
    }
   ],
   "source": [
    "print(HanLP.convertToTraditionalChinese(\"用笔记本电脑写程序\"));\n",
    "print(HanLP.convertToSimplifiedChinese(\"「以後等妳當上皇后，就能買士多啤梨慶祝了」\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[商品/n, 和/cc, 服务/vn]\n",
      "[结婚/vi, 的/ude1, 和/cc, 尚未/d, 结婚/vi, 的/ude1, 确实/ad, 在/p, 干扰/vn, 分词/n, 啊/y]\n",
      "[买/v, 水果/n, 然后/c, 来/vf, 世博园/n, 最后/f, 去/vf, 世博会/n]\n",
      "[中国/ns, 的/ude1, 首都/n, 是/vshi, 北京/ns]\n",
      "[欢迎/v, 新/a, 老/a, 师生/n, 前来/vi, 就餐/vi]\n",
      "[工信处/n, 女干事/n, 每月/t, 经过/p, 下属/v, 科室/n, 都/d, 要/v, 亲口/d, 交代/v, 24/m, 口/n, 交换机/n, 等/udeng, 技术性/n, 器件/n, 的/ude1, 安装/v, 工作/vn]\n",
      "[随着/p, 页游/nz, 兴起/v, 到/v, 现在/t, 的/ude1, 页游/nz, 繁盛/a, ，/w, 依赖于/v, 存档/vi, 进行/vn, 逻辑/n, 判断/v, 的/ude1, 设计/vn, 减少/v, 了/ule, ，/w, 但/c, 这/rzv, 块/q, 也/d, 不能/v, 完全/ad, 忽略/v, 掉/v, 。/w]\n"
     ]
    }
   ],
   "source": [
    "testCases = [\n",
    "    \"商品和服务\",\n",
    "    \"结婚的和尚未结婚的确实在干扰分词啊\",\n",
    "    \"买水果然后来世博园最后去世博会\",\n",
    "    \"中国的首都是北京\",\n",
    "    \"欢迎新老师生前来就餐\",\n",
    "    \"工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作\",\n",
    "    \"随着页游兴起到现在的页游繁盛，依赖于存档进行逻辑判断的设计减少了，但这块也不能完全忽略掉。\"]\n",
    "for sentence in testCases: \n",
    "    print(HanLP.segment(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[签约/vi, 仪式/n, 前/f, ，/w, 秦光荣/nr, 、/w, 李纪恒/nr, 、/w, 仇和/nr, 等/udeng, 一同/d, 会见/v, 了/ule, 参加/v, 签约/vi, 的/ude1, 企业家/nnt, 。/w]\n",
      "[王国强/nr, 、/w, 高峰/n, 、/w, 汪洋/n, 、/w, 张朝阳/nr, 光着头/l, 、/w, 韩寒/nr, 、/w, 小/a, 四/m]\n",
      "[张浩/nr, 和/cc, 胡健康/nr, 复员/v, 回家/vi, 了/ule]\n",
      "[王总/nr, 和/cc, 小丽/nr, 结婚/vi, 了/ule]\n",
      "[编剧/nnt, 邵钧林/nr, 和/cc, 稽道青/nr, 说/v]\n",
      "[这里/rzs, 有/vyou, 关天培/nr, 的/ude1, 有关/vn, 事迹/n]\n",
      "[龚学平/nr, 等/udeng, 领导/n, ,/w, 邓颖超/nr, 生前/t]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\n",
    "        \"签约仪式前，秦光荣、李纪恒、仇和等一同会见了参加签约的企业家。\",\n",
    "        \"王国强、高峰、汪洋、张朝阳光着头、韩寒、小四\",\n",
    "        \"张浩和胡健康复员回家了\",\n",
    "        \"王总和小丽结婚了\",\n",
    "        \"编剧邵钧林和稽道青说\",\n",
    "        \"这里有关天培的有关事迹\",\n",
    "        \"龚学平等领导,邓颖超生前\"]\n",
    "\n",
    "segment = HanLP.newSegment().enableNameRecognize(True);\n",
    "for ss in sentence:\n",
    "    print(segment.seg(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[王国强/nr, 、/w, 汪峰/nr, 、/w, 汪洋/n, 、/w, 张朝阳/nr, 光着头/l, 、/w, 韩寒/nr, 、/w, 小/a, 四/m]\n"
     ]
    }
   ],
   "source": [
    "print(HanLP.segment(\"王国强、汪峰、汪洋、张朝阳光着头、韩寒、小四\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[新北市/ns, 是/vshi, 台湾/ns, 的/ude1, 直辖市/n, ,/w, 也/d, 是/vshi, 台湾/ns, 人口/n, 最多/ad, 的/ude1, 城市/n, 。/w, 台北市/ns, 位于/v, 台湾/ns, 北部/f, ,/w, 包括/v, 台湾/ns, 北部/f, 海岸线/n, 的/ude1, 大部分/n, 延伸/v, ,/w, 并/cc, 环绕/v, 台北盆地/nz, ,/w, 使/v, 其/rz, 成为/v, 仅次于/v, 高雄/ns, 市/n, 的/ude1, 第二/mq, 大/a, 直辖市/n]\n"
     ]
    }
   ],
   "source": [
    "text = \"新北市是台灣的直轄市，也是台灣人口最多的城市。台北市位於台灣北部，包括台灣北部海岸線的大部分延伸，並環繞台北盆地，使其成為僅次於高雄市的第二大直轄市\"\n",
    "print(HanLP.segment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[张晚霞/nr, 教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n]\n",
      "[我/rr, 在/p, 渣打/nz, 国际/n, 商业银行/nis, 工作/vn]\n"
     ]
    }
   ],
   "source": [
    "text = \"张晚霞教授正在教授自然语言处理课程\"\n",
    "print(HanLP.segment(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数词和数量词识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[张晚霞/nr, 教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 123台/mq, 川/b, 喜宴/n, 餐厅/nis, 课程/n]\n",
      "[牛奶/nf, 三〇〇克/mq, */w, 2/m]\n"
     ]
    }
   ],
   "source": [
    "def demo_number_and_quantifier_recognition(sentences):\n",
    "    \"\"\" 演示数词和数量词识别\n",
    "    >>> sentences = [\n",
    "    ...    \"十九元套餐包括什么\",\n",
    "    ...    \"九千九百九十九朵玫瑰\",\n",
    "    ...    \"壹佰块都不给我\",\n",
    "    ...    \"９０１２３４５６７８只蚂蚁\",\n",
    "    ...    \"牛奶三〇〇克*2\",\n",
    "    ...    \"ChinaJoy“扫黄”细则露胸超2厘米罚款\",\n",
    "    ... ]\n",
    "    >>> demo_number_and_quantifier_recognition(sentences)\n",
    "    [十九元/mq, 套餐/n, 包括/v, 什么/ry]\n",
    "    [九千九百九十九朵/mq, 玫瑰/n]\n",
    "    [壹佰块/mq, 都/d, 不/d, 给/p, 我/rr]\n",
    "    [９０１２３４５６７８只/mq, 蚂蚁/n]\n",
    "    [牛奶/nf, 三〇〇克/mq, */w, 2/m]\n",
    "    [ChinaJoy/nx, “/w, 扫黄/vi, ”/w, 细则/n, 露/v, 胸/ng, 超/v, 2厘米/mq, 罚款/vi]\n",
    "    \"\"\"\n",
    "    StandardTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.StandardTokenizer\")\n",
    "\n",
    "    StandardTokenizer.SEGMENT.enableNumberQuantifierRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        print(StandardTokenizer.segment(sentence))\n",
    "        \n",
    "demo_number_and_quantifier_recognition([\"牛台川喜宴餐厅\"])\n",
    "text = \"牛奶三〇〇克*2\"\n",
    "print(HanLP.segment(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[张晚霞/nr, 教授/n, 正在/d, 教授/v, 自然语言处理/nz, 123/m, 台/q, 川/Ng, 喜宴/n, 餐厅/n, 课程/n]\n",
      "----\n",
      "张晚霞/人名 教授/名词 正在/副词 教授/动词 自然语言处理/其他专名 123/数词 台/量词 川/名语素 喜宴/名词 餐厅/名词 课程/名词\n",
      "----\n",
      "张晚霞/nr 教授/n 正在/d 教授/v 自然语言处理/nz 123/m 台/q 川/Ng 喜宴/n 餐厅/n 课程/n\n"
     ]
    }
   ],
   "source": [
    "def demo_NLP_segment(text):\n",
    "    \"\"\" NLP分词，更精准的中文分词、词性标注与命名实体识别\n",
    "        标注集请查阅 https://github.com/hankcs/HanLP/blob/master/data/dictionary/other/TagPKU98.csv\n",
    "        或者干脆调用 Sentence#translateLabels() 转为中文\n",
    "    >>> demo_NLP_segment()\n",
    "    [我/r, 新造/v, 一个/m, 词/n, 叫/v, 幻想乡/ns, 你/r, 能/v, 识别/v, 并/c, 正确/ad, 标注/v, 词性/n, 吗/y, ？/w]\n",
    "    我/代词 的/助词 希望/名词 是/动词 希望/动词 张晚霞/人名 的/助词 背影/名词 被/介词 晚霞/名词 映/动词 红/形容词\n",
    "    支援/v 臺灣/ns 正體/n 香港/ns 繁體/n ：/w [微软/nt 公司/n]/nt 於/p 1975年/t 由/p 比爾·蓋茲/n 和/c 保羅·艾倫/nr 創立/v 。/w\n",
    "    \"\"\"\n",
    "    NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "    print(NLPTokenizer.segment(text))  # “正确”是副形词。\n",
    "    print('----')\n",
    "    # 注意观察下面两个“希望”的词性、两个“晚霞”的词性\n",
    "    print(NLPTokenizer.analyze(text).translateLabels())\n",
    "    print('----')\n",
    "    print(NLPTokenizer.analyze(text))\n",
    "\n",
    "# text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"\n",
    "text = \"张晚霞教授正在教授自然语言处理123台川喜宴餐厅课程\"\n",
    "demo_NLP_segment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机构名识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[我/rr, 在/p, 渣打/nz, 国际/n, 商业银行/nis, 工作/vn]\n",
      "[我/rr, 在/p, 渣打国际商业银行/nt, 工作/vn]\n"
     ]
    }
   ],
   "source": [
    "def demo_organization_recognition(sentences):\n",
    "    \"\"\" 机构名识别\n",
    "    >>> sentences = [\n",
    "    ...    \"我在上海林原科技有限公司兼职工作，\",\n",
    "    ...    \"我经常在台川喜宴餐厅吃饭，\",\n",
    "    ...    \"偶尔去开元地中海影城看电影。\",\n",
    "    ... ]\n",
    "    >>> demo_organization_recognition(sentences)\n",
    "    [我/rr, 在/p, 上海/ns, 林原科技有限公司/nt, 兼职/vn, 工作/vn, ，/w]\n",
    "    [我/rr, 经常/d, 在/p, 台川喜宴餐厅/nt, 吃饭/vi, ，/w]\n",
    "    [偶尔/d, 去/vf, 开元地中海影城/nt, 看/v, 电影/n, 。/w]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "\n",
    "    segment = HanLP.newSegment().enableOrganizationRecognize(True)\n",
    "    for sentence in sentences:\n",
    "        term_list = segment.seg(sentence)\n",
    "        print(term_list)\n",
    "\n",
    "text = \"我在渣打國際商業銀行工作\"\n",
    "print(HanLP.segment(text))\n",
    "demo_organization_recognition([\"我在渣打國際商業銀行工作\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未标注： [教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n]\n",
      "标注后： [教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n]\n"
     ]
    }
   ],
   "source": [
    "# 词性标注\n",
    "def demo_pos_tagging():\n",
    "    \"\"\" 词性标注\n",
    "    >>> demo_pos_tagging()\n",
    "    未标注： [教授/nnt, 正在/d, 教授/nnt, 自然语言处理/nz, 课程/n]\n",
    "    标注后： [教授/nnt, 正在/d, 教授/v, 自然语言处理/nz, 课程/n]\n",
    "    \"\"\"\n",
    "    Segment = JClass(\"com.hankcs.hanlp.seg.Segment\")\n",
    "    text = \"教授正在教授自然语言处理课程\"\n",
    "    segment = HanLP.newSegment()\n",
    "\n",
    "    print(\"未标注：\", segment.seg(text))\n",
    "    segment.enablePartOfSpeechTagging(True)\n",
    "    print(\"标注后：\", segment.seg(text))\n",
    "\n",
    "demo_pos_tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDictionary - dictionary  have to add every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "[攻城狮/nz, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n"
     ]
    }
   ],
   "source": [
    "text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
    "\n",
    "print(HanLP.segment(text))\n",
    "\n",
    "CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "CustomDictionary.add(\"攻城狮\")  # 动态增加\n",
    "# CustomDictionary.insert(\"白富美\", \"nz 1024\")  # 强行插入\n",
    "# CustomDictionary.remove(\"攻城狮\"); # 删除词语（注释掉试试）\n",
    "# CustomDictionary.add(\"单身狗\", \"nz 1024 n 1\")\n",
    "# #print(CustomDictionary.get(\"单身狗\"))\n",
    "\n",
    "print(HanLP.segment(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 短语提取 & 自动摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "關係\n",
      "貿易\n",
      "英國政府\n",
      "議反\n",
      "援助港人\n",
      "密切關注\n",
      "萬香港\n",
      "移民政策\n",
      "政府香港\n",
      "影響\n",
      "[英國, 香港, 中國, 政府, 反, 時, 為, 恩斯伯格, 沒有, 來]\n",
      "[當時的英國政府在貿易與中國危害人權之間兩難, 此舉除反映英國政府對香港密切關注、與中關係僵化, 是英國自1984年來第二次針對香港事務召見中國大使, 此舉反映英國政府密切關注香港這個動盪的島嶼, 英國此舉也突顯與中國關係惡化]\n"
     ]
    }
   ],
   "source": [
    "# 关键词提取\n",
    "document = \"\"\"英國強烈譴責「港版國安法」，首相強生（Borris Johnson）1日也宣布援助港人措施，將允許300萬名持有英國海外國民護照（BNO）之香港公民申請入籍。BBC分析指出，此舉除反映英國政府對香港密切關注、與中關係僵化，也突顯移民政策的矛盾之處。\n",
    "BBC政治編輯庫恩斯伯格（Laura Kuenssberg）2日發表評論肯定，英國向300萬香港人開啟大門，邁出了一大步，「西敏市今晚沒有任何人預料到，會有這麼多人搬到這裡來，以躲避香港日益令人擔憂的現實。但這項決定很重要，且不僅瑾是對那些英國可能提供庇護的人。」\n",
    "她指出，此舉反映英國政府密切關注香港這個動盪的島嶼，也受到過去殖民香港的影響。香港1997年回歸中國時，便有呼籲政府給予港人英國公民身份的呼聲，但當時沒有達成。\n",
    "「請記得，權力交接是在尊重香港部分民主及市場經濟的基礎上完成，但近年來逐漸被削弱」，庫恩斯伯格說明，「值得注意的是，下議院沒有任何一名議員反對英國與香港的故事邁入下一階段。政府的決策依然受到幾十年前作出的選擇影響。」\n",
    "此外，英國此舉也突顯與中國關係惡化。庫恩斯伯格提到，「不久之前，前首相卡麥隆（David Cameron）不僅鋪紅地毯，還乘坐女王的金色馬車歡迎中國總理。」時任英國財政大臣奧斯本（George Osborne）訪中時，也罕見地對貿易的好處讚不絕口。\n",
    "當時的英國政府在貿易與中國危害人權之間兩難，再加上一群保守黨議員反對與中國建立密切關係，也認為不該讓中國電信巨擘華為（HUAWEI）參與5G網路建設。其中也有人士暗示，必須迫使政府對香港採取行動。\n",
    "中國是個不容忽視的經濟大國，也是英國官員熱衷的貿易、氣候領域合作對象。強生日前被問及此事才提到，「我的立場非常非常簡單。我不會陷入反華情緒，因為我不是個恐華者」，但中英關係確實已經產生變化。一名政府消息人士認為，英國政府態度並沒有轉硬，但「中國愈發咄咄逼人的傾向已經暴露。」\n",
    "然而，庫恩斯伯格指出，政府承諾援助港人之際，英國移民政策也面臨關鍵時期，「直到昨天為止，國會議員還在支持終結移民自由的移民法案，代表減少移民數量。而政府卻向可能到來的數百萬（香港）人發出這項訊息，這難道不矛盾嗎？」\n",
    "另一方面，根據《天空新聞》報導，英國外交部常任副部長麥唐納（Simon McDonald）1日召見中國駐英大使劉曉明，批評北京違反《中英聯合聲明》承諾，是英國自1984年來第二次針對香港事務召見中國大使。劉曉明則反批英國干涉內政，雙方氣氛緊張。\"\"\"\n",
    "phraseList = HanLP.extractPhrase(document, 10);\n",
    "for pp in phraseList:\n",
    "    print(pp)\n",
    "# 关键词提取\n",
    "print(HanLP.extractKeyword(document, 10))\n",
    "# 自动摘要\n",
    "print(HanLP.extractSummary(document, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[水资源, 陈明忠]\n",
      "[严格地进行水资源论证和取水许可的批准, 水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露, 有部分省超过红线的指标]\n",
      "1\t徐先生\t徐先生\tnh\tnr\t_\t4\t主谓关系\t_\t_\n",
      "2\t还\t还\td\td\t_\t4\t状中结构\t_\t_\n",
      "3\t具体\t具体\ta\tad\t_\t4\t状中结构\t_\t_\n",
      "4\t帮助\t帮助\tv\tv\t_\t0\t核心关系\t_\t_\n",
      "5\t他\t他\tr\tr\t_\t4\t兼语\t_\t_\n",
      "6\t确定\t确定\tv\tv\t_\t4\t动宾关系\t_\t_\n",
      "7\t了\t了\tu\tu\t_\t6\t右附加关系\t_\t_\n",
      "8\t把\t把\tp\tp\t_\t15\t状中结构\t_\t_\n",
      "9\t画\t画\tv\tv\t_\t8\t介宾关系\t_\t_\n",
      "10\t雄鹰\t雄鹰\tn\tn\t_\t9\t动宾关系\t_\t_\n",
      "11\t、\t、\twp\tw\t_\t12\t标点符号\t_\t_\n",
      "12\t松鼠\t松鼠\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "13\t和\t和\tc\tc\t_\t14\t左附加关系\t_\t_\n",
      "14\t麻雀\t麻雀\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "15\t作为\t作为\tv\tv\t_\t6\t动宾关系\t_\t_\n",
      "16\t主攻\t主攻\tv\tvn\t_\t17\t定中关系\t_\t_\n",
      "17\t目标\t目标\tn\tn\t_\t15\t动宾关系\t_\t_\n",
      "18\t。\t。\twp\tw\t_\t4\t标点符号\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 关键词提取\n",
    "document = \"水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，\" \\\n",
    "           \"根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，\" \\\n",
    "           \"有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，\" \\\n",
    "           \"严格地进行水资源论证和取水许可的批准。\"\n",
    "print(HanLP.extractKeyword(document, 2))\n",
    "# 自动摘要\n",
    "print(HanLP.extractSummary(document, 3))\n",
    "# 依存句法分析\n",
    "# print(HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拼音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xin1\n",
      "bei3\n",
      "shi4\n",
      "shi4\n",
      "tai2\n",
      "wan1\n",
      "de5\n",
      "zhi2\n",
      "xia2\n",
      "shi4\n",
      "-------------------------\n",
      "1\n",
      "3\n",
      "4\n",
      "4\n",
      "2\n",
      "1\n",
      "5\n",
      "2\n",
      "2\n",
      "4\n",
      "-------------------------\n",
      "xin\n",
      "bei\n",
      "shi\n",
      "shi\n",
      "tai\n",
      "wan\n",
      "de\n",
      "zhi\n",
      "xia\n",
      "shi\n"
     ]
    }
   ],
   "source": [
    "text = \"新北市是台灣的直轄市\"\n",
    "pinyinList = HanLP.convertToPinyinList(text)\n",
    "for pp in pinyinList:\n",
    "    print(pp) #拼音（数字音调\n",
    "print('-------------------------')\n",
    "for pp in pinyinList:\n",
    "    print(pp.getTone())\n",
    "print('-------------------------')\n",
    "for pp in pinyinList:\n",
    "    print(pp.getPinyinWithoutTone())\n",
    "    \n",
    "# getPinyinWithToneMark 拼音（符号音调）\n",
    "# getPinyinWithoutTone 拼音（无音调）\n",
    "# getTone 声调\n",
    "# getShengmu 声母\n",
    "# getYunmu 韵母\n",
    "# getHead 输入法头\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "夫措施可以动用同义词词典将同段落文本改写成意思相似的外一样段落文本，而且大多符合语法\n"
     ]
    }
   ],
   "source": [
    "CoreSynonymDictionary = JClass(\"com.hankcs.hanlp.dictionary.CoreSynonymDictionary\")\n",
    "text = \"这个方法可以利用同义词词典将一段文本改写成意思相似的另一段文本，而且差不多符合语法\"\n",
    "print(CoreSynonymDictionary.rewrite(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本推荐(句子级别，从一系列句子中挑出与输入句子最相似的那一个)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[威廉王子发表演说 呼吁保护野生动物, 英报告说空气污染带来“公共健康危机”]\n",
      "[英报告说空气污染带来“公共健康危机”]\n",
      "[《时代》年度人物最终入围名单出炉 普京马云入选]\n",
      "[魅惑天后许佳慧不爱“预谋” 独唱《许某某》]\n"
     ]
    }
   ],
   "source": [
    "def demo_suggester():\n",
    "    \"\"\" 文本推荐(句子级别，从一系列句子中挑出与输入句子最相似的那一个)\n",
    "    >>> demo_suggester()\n",
    "    [威廉王子发表演说 呼吁保护野生动物, 英报告说空气污染带来“公共健康危机”]\n",
    "    [英报告说空气污染带来“公共健康危机”]\n",
    "    [《时代》年度人物最终入围名单出炉 普京马云入选]\n",
    "    [魅惑天后许佳慧不爱“预谋” 独唱《许某某》]\n",
    "    \"\"\"\n",
    "    Suggester = JClass(\"com.hankcs.hanlp.suggest.Suggester\")\n",
    "    suggester = Suggester()\n",
    "    title_array = [\n",
    "        \"威廉王子发表演说 呼吁保护野生动物\",\n",
    "        \"魅惑天后许佳慧不爱“预谋” 独唱《许某某》\",\n",
    "        \"《时代》年度人物最终入围名单出炉 普京马云入选\",\n",
    "        \"“黑格比”横扫菲：菲吸取“海燕”经验及早疏散\",\n",
    "        \"日本保密法将正式生效 日媒指其损害国民知情权\",\n",
    "        \"英报告说空气污染带来“公共健康危机”\"\n",
    "    ]\n",
    "    for title in title_array:\n",
    "        suggester.addSentence(title)\n",
    "\n",
    "    print(suggester.suggest(\"陈述\", 2))      # 语义\n",
    "    print(suggester.suggest(\"危机公关\", 1))  # 字符\n",
    "    print(suggester.suggest(\"mayun\", 1))   # 拼音\n",
    "    print(suggester.suggest(\"徐家汇\", 1)) # 拼音\n",
    "        \n",
    "demo_suggester()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语义相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词A   \t词B   \t语义距离      \t语义相似度\n",
      "\n",
      "香蕉   \t香蕉   \t0              \t1.0000000000\n",
      "香蕉   \t苹果   \t19980          \t0.9999997311\n",
      "香蕉   \t水果   \t32967          \t0.9999995563\n",
      "香蕉   \t蔬菜   \t2630367        \t0.9999645975\n",
      "香蕉   \t自行车  \t1854515628     \t0.9750398066\n",
      "香蕉   \t公交车  \t1854535619     \t0.9750395376\n",
      "苹果   \t香蕉   \t19980          \t0.9999997311\n",
      "苹果   \t苹果   \t0              \t1.0000000000\n",
      "苹果   \t水果   \t12987          \t0.9999998252\n",
      "苹果   \t蔬菜   \t2610387        \t0.9999648664\n",
      "苹果   \t自行车  \t1854535608     \t0.9750395377\n",
      "苹果   \t公交车  \t1854555599     \t0.9750392686\n",
      "水果   \t香蕉   \t32967          \t0.9999995563\n",
      "水果   \t苹果   \t12987          \t0.9999998252\n",
      "水果   \t水果   \t0              \t1.0000000000\n",
      "水果   \t蔬菜   \t2597400        \t0.9999650412\n",
      "水果   \t自行车  \t1854548595     \t0.9750393629\n",
      "水果   \t公交车  \t1854568586     \t0.9750390939\n",
      "蔬菜   \t香蕉   \t2630367        \t0.9999645975\n",
      "蔬菜   \t苹果   \t2610387        \t0.9999648664\n",
      "蔬菜   \t水果   \t2597400        \t0.9999650412\n",
      "蔬菜   \t蔬菜   \t0              \t1.0000000000\n",
      "蔬菜   \t自行车  \t1857145995     \t0.9750044041\n",
      "蔬菜   \t公交车  \t1857165986     \t0.9750041351\n",
      "自行车  \t香蕉   \t1854515628     \t0.9750398066\n",
      "自行车  \t苹果   \t1854535608     \t0.9750395377\n",
      "自行车  \t水果   \t1854548595     \t0.9750393629\n",
      "自行车  \t蔬菜   \t1857145995     \t0.9750044041\n",
      "自行车  \t自行车  \t0              \t1.0000000000\n",
      "自行车  \t公交车  \t19991          \t0.9999997309\n",
      "公交车  \t香蕉   \t1854535619     \t0.9750395376\n",
      "公交车  \t苹果   \t1854555599     \t0.9750392686\n",
      "公交车  \t水果   \t1854568586     \t0.9750390939\n",
      "公交车  \t蔬菜   \t1857165986     \t0.9750041351\n",
      "公交车  \t自行车  \t19991          \t0.9999997309\n",
      "公交车  \t公交车  \t0              \t1.0000000000\n"
     ]
    }
   ],
   "source": [
    "CoreSynonymDictionary = JClass(\"com.hankcs.hanlp.dictionary.CoreSynonymDictionary\")\n",
    "\n",
    "word_array = [\n",
    "    \"香蕉\",\n",
    "    \"苹果\",\n",
    "    \"水果\",\n",
    "    \"蔬菜\",\n",
    "    \"自行车\",\n",
    "    \"公交车\",\n",
    "]\n",
    "print(\"%-5s\\t%-5s\\t%-10s\\t%-5s\\n\" % (\"词A\", \"词B\", \"语义距离\", \"语义相似度\"))\n",
    "for a in word_array:\n",
    "    for b in word_array:\n",
    "        print(\"%-5s\\t%-5s\\t%-15d\\t%-5.10f\" % (a, b, CoreSynonymDictionary.distance(a, b),\n",
    "            CoreSynonymDictionary.similarity(a, b)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[张三, 马六], [李四, 钱二], [王五, 赵一]]\n",
      "[[李四, 钱二], [王五, 赵一], [张三, 马六]]\n",
      "[[李四, 钱二], [王五, 赵一], [张三, 马六]]\n"
     ]
    }
   ],
   "source": [
    "ClusterAnalyzer = JClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')\n",
    "analyzer = ClusterAnalyzer()\n",
    "analyzer.addDocument(\"赵一\", \"流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 摇滚, 摇滚, 摇滚, 摇滚\")\n",
    "analyzer.addDocument(\"钱二\", \"爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲\")\n",
    "analyzer.addDocument(\"张三\", \"古典, 古典, 古典, 古典, 民谣, 民谣, 民谣, 民谣\")\n",
    "analyzer.addDocument(\"李四\", \"爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 金属, 金属, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲\")\n",
    "analyzer.addDocument(\"王五\", \"流行, 流行, 流行, 流行, 摇滚, 摇滚, 摇滚, 嘻哈, 嘻哈, 嘻哈\")\n",
    "analyzer.addDocument(\"马六\", \"古典, 古典, 古典, 古典, 古典, 古典, 古典, 古典, 摇滚\")\n",
    "print(analyzer.kmeans(3))\n",
    "print(analyzer.repeatedBisection(3))\n",
    "print(analyzer.repeatedBisection(1.0))  # 自动判断聚类数量k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# url recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HanLP/nx, 的/ude1, 项目/n, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/xu, ，/w, \n",
      "    /w, .../w,  /w, 发布/v, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/releases/xu, ，/w, \n",
      "    /w, .../w,  /w, 我/rr, 有时候/d, 会/v, 在/p, www/nx, ./w, hankcs/nrf, ./w, com/nx, 上面/f, 发布/v, 一些/m, 消息/n, ，/w, \n",
      "    /w, .../w,  /w, 我/rr, 的/ude1, 微博/n, 是/vshi, http://weibo.com/hankcs/xu, //w, ，/w, 会/v, 同步/vd, 推送/nz, hankcs/nrf, ./w, com/nx, 的/ude1, 新闻/n, 。/w, \n",
      "    /w, .../w,  /w, 听说/v, ./w, 中国/ns, 域名/n, 开放/v, 申请/v, 了/ule, ,/w, 但/c, 我/rr, 并/cc, 没有/v, 申请/v, hankcs/nrf, ./w, 中国/ns, ,/w, 因为/c, 穷/a, ……/w, \n",
      "    /w, .../w,  /w]\n",
      "https://github.com/hankcs/HanLP\n",
      "https://github.com/hankcs/HanLP/releases\n",
      "http://weibo.com/hankcs\n"
     ]
    }
   ],
   "source": [
    "def demo_URL_recognition(text):\n",
    "    \"\"\" 演示URL识别\n",
    "    >>> text = '''HanLP的项目地址是https://github.com/hankcs/HanLP，\n",
    "    ... 发布地址是https://github.com/hankcs/HanLP/releases，\n",
    "    ... 我有时候会在www.hankcs.com上面发布一些消息，\n",
    "    ... 我的微博是http://weibo.com/hankcs/，会同步推送hankcs.com的新闻。\n",
    "    ... 听说.中国域名开放申请了,但我并没有申请hankcs.中国,因为穷……\n",
    "    ... '''\n",
    "    >>> demo_URL_recognition(text)\n",
    "    [HanLP/nx, 的/ude1, 项目/n, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/xu, ，/w,\n",
    "    /w, 发布/v, 地址/n, 是/vshi, https://github.com/hankcs/HanLP/releases/xu, ，/w,\n",
    "    /w, 我/rr, 有时候/d, 会/v, 在/p, www.hankcs.com/xu, 上面/f, 发布/v, 一些/m, 消息/n, ，/w,\n",
    "    /w, 我/rr, 的/ude1, 微博/n, 是/vshi, http://weibo.com/hankcs//xu, ，/w, 会/v,\n",
    "        同步/vd, 推送/nz, hankcs.com/xu, 的/ude1, 新闻/n, 。/w,\n",
    "    /w, 听说/v, ./w, 中国/ns, 域名/n, 开放/v, 申请/v, 了/ule, ,/w, 但/c, 我/rr, 并/cc,\n",
    "        没有/v, 申请/v, hankcs.中国/xu, ,/w, 因为/c, 穷/a, ……/w,\n",
    "    /w]\n",
    "    https://github.com/hankcs/HanLP\n",
    "    https://github.com/hankcs/HanLP/releases\n",
    "    www.hankcs.com\n",
    "    http://weibo.com/hankcs/\n",
    "    hankcs.com\n",
    "    hankcs.中国\n",
    "    \"\"\"\n",
    "    Nature = JClass(\"com.hankcs.hanlp.corpus.tag.Nature\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    URLTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.URLTokenizer\")\n",
    "\n",
    "    term_list = URLTokenizer.segment(text)\n",
    "    print(term_list)\n",
    "    for term in term_list:\n",
    "        if term.nature == Nature.xu:\n",
    "            print(term.word)\n",
    "\n",
    "text = '''HanLP的项目地址是https://github.com/hankcs/HanLP，\n",
    "    ... 发布地址是https://github.com/hankcs/HanLP/releases，\n",
    "    ... 我有时候会在www.hankcs.com上面发布一些消息，\n",
    "    ... 我的微博是http://weibo.com/hankcs/，会同步推送hankcs.com的新闻。\n",
    "    ... 听说.中国域名开放申请了,但我并没有申请hankcs.中国,因为穷……\n",
    "    ... '''\n",
    "demo_URL_recognition(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stopwprd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
      "[小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz]\n",
      "[居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n"
     ]
    }
   ],
   "source": [
    "def demo_notional_tokenizer():\n",
    "    \"\"\" 演示自动去除停用词、自动断句的分词器\n",
    "    >>> demo_notional_tokenizer()\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz]\n",
    "    [居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    \"\"\"\n",
    "    Term =JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    NotionalTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NotionalTokenizer\")\n",
    "\n",
    "    text = \"小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝\"\n",
    "    print(NotionalTokenizer.segment(text))\n",
    "    for sentence in NotionalTokenizer.seg2sentence(text):\n",
    "        print(sentence)\n",
    "        \n",
    "demo_notional_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正规化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[爱听4g/nz]\n",
      "[爱听4g/nz]\n",
      "[爱听4g/nz]\n",
      "[爱听4g/nz]\n",
      "[爱听4g/nz]\n"
     ]
    }
   ],
   "source": [
    "def demo_normalization():\n",
    "    \"\"\" 演示正规化字符配置项的效果（繁体->简体，全角->半角，大写->小写）。\n",
    "        该配置项位于hanlp.properties中，通过Normalization=true来开启\n",
    "        切换配置后必须删除CustomDictionary.txt.bin缓存，否则只影响动态插入的新词。\n",
    "    >>> demo_normalization()\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    [爱听4g/nz]\n",
    "    \"\"\"\n",
    "    CustomDictionary =JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "    Config.Normalization = True\n",
    "    CustomDictionary.insert(\"爱听4G\", \"nz 1000\")\n",
    "    print(HanLP.segment(\"爱听4g\"))\n",
    "    print(HanLP.segment(\"爱听4G\"))\n",
    "    print(HanLP.segment(\"爱听４G\"))\n",
    "    print(HanLP.segment(\"爱听４Ｇ\"))\n",
    "    print(HanLP.segment(\"愛聽４Ｇ\"))\n",
    "    CustomDictionary.remove(\"爱听4G\")\n",
    "    \n",
    "demo_normalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "徐先生 --(主谓关系)--> 帮助\n",
      "还 --(状中结构)--> 帮助\n",
      "具体 --(状中结构)--> 帮助\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "他 --(兼语)--> 帮助\n",
      "确定 --(动宾关系)--> 帮助\n",
      "了 --(右附加关系)--> 确定\n",
      "把 --(状中结构)--> 作为\n",
      "画 --(介宾关系)--> 把\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "、 --(标点符号)--> 松鼠\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "和 --(左附加关系)--> 麻雀\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "作为 --(动宾关系)--> 确定\n",
      "主攻 --(定中关系)--> 目标\n",
      "目标 --(动宾关系)--> 作为\n",
      "。 --(标点符号)--> 帮助\n",
      "\n",
      "徐先生 --(主谓关系)--> 帮助\n",
      "还 --(状中结构)--> 帮助\n",
      "具体 --(状中结构)--> 帮助\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "他 --(兼语)--> 帮助\n",
      "确定 --(动宾关系)--> 帮助\n",
      "了 --(右附加关系)--> 确定\n",
      "把 --(状中结构)--> 作为\n",
      "画 --(介宾关系)--> 把\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "、 --(标点符号)--> 松鼠\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "和 --(左附加关系)--> 麻雀\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "作为 --(动宾关系)--> 确定\n",
      "主攻 --(定中关系)--> 目标\n",
      "目标 --(动宾关系)--> 作为\n",
      "。 --(标点符号)--> 帮助\n",
      "\n",
      "麻雀 --(并列关系)--> \n",
      "雄鹰 --(动宾关系)--> \n",
      "画 --(介宾关系)--> \n",
      "把 --(状中结构)--> \n",
      "作为 --(动宾关系)--> \n",
      "确定 --(动宾关系)--> \n",
      "帮助 --(核心关系)--> \n",
      "##核心##\n"
     ]
    }
   ],
   "source": [
    "def demo_dependency_parser():\n",
    "    \"\"\" 依存句法分析（CRF句法模型需要-Xms512m -Xmx512m -Xmn256m，\n",
    "        MaxEnt和神经网络句法模型需要-Xms1g -Xmx1g -Xmn512m）\n",
    "    >>> demo_dependency_parser()\n",
    "    徐先生 --(主谓关系)--> 帮助\n",
    "    还 --(状中结构)--> 帮助\n",
    "    具体 --(状中结构)--> 帮助\n",
    "    帮助 --(核心关系)--> ##核心##\n",
    "    他 --(兼语)--> 帮助\n",
    "    确定 --(动宾关系)--> 帮助\n",
    "    了 --(右附加关系)--> 确定\n",
    "    把 --(状中结构)--> 作为\n",
    "    画 --(介宾关系)--> 把\n",
    "    雄鹰 --(动宾关系)--> 画\n",
    "    、 --(标点符号)--> 松鼠\n",
    "    松鼠 --(并列关系)--> 雄鹰\n",
    "    和 --(左附加关系)--> 麻雀\n",
    "    麻雀 --(并列关系)--> 雄鹰\n",
    "    作为 --(动宾关系)--> 确定\n",
    "    主攻 --(定中关系)--> 目标\n",
    "    目标 --(动宾关系)--> 作为\n",
    "    。 --(标点符号)--> 帮助\n",
    "    <BLANKLINE>\n",
    "    徐先生 --(主谓关系)--> 帮助\n",
    "    还 --(状中结构)--> 帮助\n",
    "    具体 --(状中结构)--> 帮助\n",
    "    帮助 --(核心关系)--> ##核心##\n",
    "    他 --(兼语)--> 帮助\n",
    "    确定 --(动宾关系)--> 帮助\n",
    "    了 --(右附加关系)--> 确定\n",
    "    把 --(状中结构)--> 作为\n",
    "    画 --(介宾关系)--> 把\n",
    "    雄鹰 --(动宾关系)--> 画\n",
    "    、 --(标点符号)--> 松鼠\n",
    "    松鼠 --(并列关系)--> 雄鹰\n",
    "    和 --(左附加关系)--> 麻雀\n",
    "    麻雀 --(并列关系)--> 雄鹰\n",
    "    作为 --(动宾关系)--> 确定\n",
    "    主攻 --(定中关系)--> 目标\n",
    "    目标 --(动宾关系)--> 作为\n",
    "    。 --(标点符号)--> 帮助\n",
    "    <BLANKLINE>\n",
    "    麻雀 --(并列关系)-->\n",
    "    雄鹰 --(动宾关系)-->\n",
    "    画 --(介宾关系)-->\n",
    "    把 --(状中结构)-->\n",
    "    作为 --(动宾关系)-->\n",
    "    确定 --(动宾关系)-->\n",
    "    帮助 --(核心关系)-->\n",
    "    ##核心##\n",
    "    \"\"\"\n",
    "    sentence = HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\")\n",
    "    for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "        print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "    print()\n",
    "\n",
    "    # 也可以直接拿到数组，任意顺序或逆序遍历\n",
    "    word_array = sentence.getWordArray()\n",
    "    for word in word_array:\n",
    "        print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "    print()\n",
    "\n",
    "    # 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根\n",
    "    CoNLLWord = JClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\")\n",
    "    head = word_array[12]\n",
    "    while head.HEAD:\n",
    "        head = head.HEAD\n",
    "        if (head == CoNLLWord.ROOT):\n",
    "            print(head.LEMMA)\n",
    "        else:\n",
    "            print(\"%s --(%s)--> \" % (head.LEMMA, head.DEPREL))\n",
    "            \n",
    "demo_dependency_parser()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
